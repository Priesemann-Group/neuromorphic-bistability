@article{ahn_link_2010,
  title = {Link Communities Reveal Multiscale Complexity in Networks},
  author = {Ahn, Yong-Yeol and Bagrow, James P. and Lehmann, Sune},
  year = {2010},
  month = aug,
  journal = {Nature},
  volume = {466},
  number = {7307},
  pages = {761--764},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature09182},
  urldate = {2021-03-12},
  abstract = {Network theory has become pervasive in all sectors of biology, from biochemical signalling patterns to the structure of human societies, but it has proved difficult to identify relevant functional communities because many nodes belong to several overlapping groups at once and are involved in hierarchical structures. Ahn et al., rather than considering nodes combining to form a tree or dendrogram as the natural community structure, suggest that communities of linkage can explain both overlap and hierarchical organization in networks. They use mobile-phone company records, representing the call patterns and locations of millions of users, to show that branches of the hierarchy are geographically correlated at multiple levels (neighbourhood, city and region) while keeping significant overlap. Linkage dendograms of this type are also demonstrated in published data on protein\textendash protein interactions and metabolic networks.},
  copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english}
}

@article{akil_synaptic_2020,
  title = {Synaptic {{Plasticity}} in {{Correlated Balanced Networks}}},
  author = {Akil, Alan Eric and Rosenbaum, Robert and Josi{\'c}, Kre{\v s}imir},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.12453},
  eprint = {2004.12453},
  urldate = {2020-04-29},
  abstract = {The dynamics of local cortical networks are irregular, but correlated. Dynamic excitatory--inhibitory balance is a plausible mechanism that generates such irregular activity, but it remains unclear how balance is achieved and maintained in plastic neural networks. In particular, it is not fully understood how plasticity induced changes in the network affect balance, and in turn, how correlated, balanced activity impacts learning. How does the dynamics of balanced networks change under different plasticity rules? How does correlated spiking activity in recurrent networks change the evolution of weights, their eventual magnitude, and structure across the network? To address these questions, we develop a general theory of plasticity in balanced networks. We show that balance can be attained and maintained under plasticity induced weight changes. We find that correlations in the input mildly, but significantly affect the evolution of synaptic weights. Under certain plasticity rules, we find an emergence of correlations between firing rates and synaptic weights. Under these rules, synaptic weights converge to a stable manifold in weight space with their final configuration dependent on the initial state of the network. Lastly, we show that our framework can also describe the dynamics of plastic balanced networks when subsets of neurons receive targeted optogenetic input.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Quantitative Biology - Neurons and Cognition}
}

@article{bak_self-organized_1988,
  title = {Self-Organized Criticality},
  author = {Bak, Per and Tang, Chao and Wiesenfeld, Kurt},
  year = {1988},
  month = jul,
  journal = {Phys. Rev. A},
  volume = {38},
  number = {1},
  pages = {364--374},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevA.38.364},
  urldate = {2022-03-25},
  abstract = {We show that certain extended dissipative dynamical systems naturally evolve into a critical state, with no characteristic time or length scales. The temporal ``fingerprint'' of the self-organized critical state is the presence of flicker noise or 1/f noise; its spatial signature is the emergence of scale-invariant (fractal) structure.}
}

@article{baker_correlated_2019,
  title = {Correlated States in Balanced Neuronal Networks},
  author = {Baker, Cody and Ebsch, Christopher and Lampl, Ilan and Rosenbaum, Robert},
  year = {2019},
  month = may,
  journal = {Phys. Rev. E},
  volume = {99},
  number = {5},
  pages = {052414},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.99.052414},
  urldate = {2022-01-19},
  abstract = {Understanding the magnitude and structure of interneuronal correlations and their relationship to synaptic connectivity structure is an important and difficult problem in computational neuroscience. Early studies show that neuronal network models with excitatory-inhibitory balance naturally create very weak spike train correlations, defining the ``asynchronous state.'' Later work showed that, under some connectivity structures, balanced networks can produce larger correlations between some neuron pairs, even when the average correlation is very small. All of these previous studies assume that the local network receives feedforward synaptic input from a population of uncorrelated spike trains. We show that when spike trains providing feedforward input are correlated, the downstream recurrent network produces much larger correlations. We provide an in-depth analysis of the resulting ``correlated state'' in balanced networks and show that, unlike the asynchronous state, it produces a tight excitatory-inhibitory balance consistent with in vivo cortical recordings.}
}

@article{bar_discrete_2000,
  title = {Discrete {{Stochastic Modeling}} of {{Calcium Channel Dynamics}}},
  author = {B{\"a}r, Markus and Falcke, Martin and Levine, Herbert and Tsimring, Lev S.},
  year = {2000},
  month = jun,
  journal = {Phys. Rev. Lett.},
  volume = {84},
  number = {24},
  pages = {5664--5667},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.84.5664},
  urldate = {2022-02-09},
  abstract = {We propose a discrete stochastic model for calcium dynamics in living cells. A set of probabilities for the opening/closing of calcium channels is assumed to depend on the calcium concentration. We study this model in one dimension, analytically in the limit of a large number of channels per site N, and numerically for small N. As the number of channels per site is increased, the transition from a nonpropagating region of activity to a propagating one changes from one described by directed percolation to that of deterministic depinning in a spatially discrete system. Also, for a small number of channels a propagating calcium wave can leave behind a novel fluctuation-driven state.}
}

@article{beggs_neuronal_2003,
  title = {Neuronal {{Avalanches}} in {{Neocortical Circuits}}},
  author = {Beggs, John M. and Plenz, Dietmar},
  year = {2003},
  month = dec,
  journal = {J. Neurosci.},
  volume = {23},
  number = {35},
  pages = {11167--11177},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-35-11167.2003},
  urldate = {2018-08-03},
  abstract = {Networks of living neurons exhibit diverse patterns of activity, including oscillations, synchrony, and waves. Recent work in physics has shown yet another mode of activity in systems composed of many nonlinear units interacting locally. For example, avalanches, earthquakes, and forest fires all propagate in systems organized into a critical state in which event sizes show no characteristic scale and are described by power laws. We hypothesized that a similar mode of activity with complex emergent properties could exist in networks of cortical neurons. We investigated this issue in mature organotypic cultures and acute slices of rat cortex by recording spontaneous local field potentials continuously using a 60 channel multielectrode array. Here, we show that propagation of spontaneous activity in cortical networks is described by equations that govern avalanches. As predicted by theory for a critical branching process, the propagation obeys a power law with an exponent of -3/2 for event sizes, with a branching parameter close to the critical value of 1. Simulations show that a branching parameter at this value optimizes information transmission in feedforward networks, while preventing runaway network excitation. Our findings suggest that ``neuronal avalanches'' may be a generic property of cortical networks, and represent a mode of activity that differs profoundly from oscillatory, synchronized, or wave-like network states. In the critical state, the network may satisfy the competing demands of information transmission and network stability.},
  copyright = {Copyright \textcopyright{} 2003 Society for Neuroscience 0270-6474/03/2311167-11.00/0},
  langid = {english},
  pmid = {14657176},
  keywords = {branching process,cortex,multielectrode array,organotypic culture,power law,self-organized criticality}
}

@article{benitez_langevin_2016,
  title = {Langevin {{Equations}} for {{Reaction-Diffusion Processes}}},
  author = {Benitez, Federico and Duclut, Charlie and Chat{\'e}, Hugues and Delamotte, Bertrand and Dornic, Ivan and Mu{\~n}oz, Miguel A.},
  year = {2016},
  month = sep,
  journal = {Phys. Rev. Lett.},
  volume = {117},
  number = {10},
  pages = {100601},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.117.100601},
  urldate = {2021-12-07},
  langid = {english}
}

@article{benjamin_neurogrid_2014,
  title = {Neurogrid: {{A Mixed-Analog-Digital Multichip System}} for {{Large-Scale Neural Simulations}}},
  shorttitle = {Neurogrid},
  author = {Benjamin, Ben Varkey and Gao, Peiran and McQuinn, Emmett and Choudhary, Swadesh and Chandrasekaran, Anand R. and Bussat, Jean-Marie and {Alvarez-Icaza}, Rodrigo and Arthur, John V. and Merolla, Paul A. and Boahen, Kwabena},
  year = {2014},
  month = may,
  journal = {Proc. IEEE},
  volume = {102},
  number = {5},
  pages = {699--716},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2014.2313565},
  abstract = {In this paper, we describe the design of Neurogrid, a neuromorphic system for simulating large-scale neural models in real time. Neuromorphic systems realize the function of biological neural systems by emulating their structure. Designers of such systems face three major design choices: 1) whether to emulate the four neural elements-axonal arbor, synapse, dendritic tree, and soma-with dedicated or shared electronic circuits; 2) whether to implement these electronic circuits in an analog or digital manner; and 3) whether to interconnect arrays of these silicon neurons with a mesh or a tree network. The choices we made were: 1) we emulated all neural elements except the soma with shared electronic circuits; this choice maximized the number of synaptic connections; 2) we realized all electronic circuits except those for axonal arbors in an analog manner; this choice maximized energy efficiency; and 3) we interconnected neural arrays in a tree network; this choice maximized throughput. These three choices made it possible to simulate a million neurons with billions of synaptic connections in real time-for the first time-using 16 Neurocores integrated on a board that consumes three watts.},
  keywords = {Analog circuits,application specific integrated circuits,asynchronous circuits,brain modeling,computational neuroscience,Computer architecture,Electronic circuits,Integrated circuit modeling,interconnection networks,mixed analog-digital integrated circuits,Nerve fibers,neural network hardware,Neural networks,neuromorphic electronic systems,Neuroscience,Random access memory,Synchronous digital hierarchy}
}

@article{biancalani_noise-induced_2014,
  title = {Noise-{{Induced Bistable States}} and {{Their Mean Switching Time}} in {{Foraging Colonies}}},
  author = {Biancalani, Tommaso and Dyson, Louise and McKane, Alan J.},
  year = {2014},
  month = jan,
  journal = {Phys. Rev. Lett.},
  volume = {112},
  number = {3},
  pages = {038101},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.112.038101},
  urldate = {2021-09-17},
  langid = {english}
}

@article{bonachela_self-organization_2010,
  title = {Self-Organization without Conservation: Are Neuronal Avalanches Generically Critical?},
  shorttitle = {Self-Organization without Conservation},
  author = {Bonachela, Juan A. and de Franciscis, Sebastiano and Torres, Joaqu{\'i}n J. and Mu{\~n}oz, Miguel A.},
  year = {2010},
  month = feb,
  journal = {J. Stat. Mech. Theory Exp.},
  volume = {2010},
  number = {02},
  pages = {P02015},
  publisher = {{IOP Publishing}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2010/02/P02015},
  urldate = {2022-03-25},
  abstract = {Recent experiments on cortical neural networks have revealed the existence of well-defined avalanches of electrical activity. Such avalanches have been claimed to be generically scale invariant\textemdash i.e. power law distributed\textemdash with many exciting implications in neuroscience. Recently, a self-organized model has been proposed by Levina, Herrmann and Geisel to explain this empirical finding. Given that (i) neural dynamics is dissipative and (ii) there is a loading mechanism progressively `charging' the background synaptic strength, this model/dynamics is very similar in spirit to forest-fire and earthquake models, archetypical examples of non-conserving self-organization, which have recently been shown to lack true criticality. Here we show that cortical neural networks obeying (i) and (ii) are not generically critical; unless parameters are fine-tuned, their dynamics is either subcritical or supercritical, even if the pseudo-critical region is relatively broad. This conclusion seems to be in agreement with the most recent experimental observations. The main implication of our work is that, if future experimental research on cortical networks were to support the observation that truly critical avalanches are the norm and not the exception, then one should look for more elaborate (adaptive/evolutionary) explanations, beyond simple self-organization, to account for this.},
  langid = {english}
}

@article{bottcher_critical_2017,
  title = {Critical {{Behaviors}} in {{Contagion Dynamics}}},
  author = {B{\"o}ttcher, L. and Nagler, J. and Herrmann, H. J.},
  year = {2017},
  month = feb,
  journal = {Phys. Rev. Lett.},
  volume = {118},
  number = {8},
  pages = {088301},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.118.088301},
  urldate = {2022-01-05},
  abstract = {We study the critical behavior of a general contagion model where nodes are either active (e.g., with opinion A, or functioning) or inactive (e.g., with opinion B, or damaged). The transitions between these two states are determined by (i) spontaneous transitions independent of the neighborhood, (ii) transitions induced by neighboring nodes, and (iii) spontaneous reverse transitions. The resulting dynamics is extremely rich including limit cycles and random phase switching. We derive a unifying mean-field theory. Specifically, we analytically show that the critical behavior of systems whose dynamics is governed by processes (i)\textendash (iii) can only exhibit three distinct regimes: (a) uncorrelated spontaneous transition dynamics, (b) contact process dynamics, and (c) cusp catastrophes. This ends a long-standing debate on the universality classes of complex contagion dynamics in mean field and substantially deepens its mathematical understanding.}
}

@article{brooks_probability_1963,
  title = {A {{Probability Theorem}} for {{Random Two-Valued Functions}}, with {{Application}} to {{Autocorrelations}}},
  author = {Brooks, Foster and Diamantides, N. D.},
  year = {1963},
  month = jan,
  journal = {SIAM Rev.},
  volume = {5},
  number = {1},
  pages = {33--40},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/1005003},
  urldate = {2021-10-12},
  langid = {english}
}

@article{brunel_dynamics_2000,
  title = {Dynamics of {{Sparsely Connected Networks}} of {{Excitatory}} and {{Inhibitory Spiking Neurons}}},
  author = {Brunel, Nicolas},
  year = {2000},
  month = may,
  journal = {J. Comput. Neurosci.},
  volume = {8},
  number = {3},
  pages = {183--208},
  issn = {1573-6873},
  doi = {10.1023/A:1008925309027},
  urldate = {2021-12-29},
  abstract = {The dynamics of networks of sparsely connected excitatory and inhibitory integrate-and-fire neurons are studied analytically. The analysis reveals a rich repertoire of states, including synchronous states in which neurons fire regularly; asynchronous states with stationary global activity and very irregular individual cell activity; and states in which the global activity oscillates but individual cells fire irregularly, typically at rates lower than the global oscillation frequency. The network can switch between these states, provided the external frequency, or the balance between excitation and inhibition, is varied. Two types of network oscillations are observed. In the fast oscillatory state, the network frequency is almost fully controlled by the synaptic time scale. In the slow oscillatory state, the network frequency depends mostly on the membrane time constant. Finite size effects in the asynchronous state are also discussed.},
  langid = {english}
}

@article{brunel_effects_2001,
  title = {Effects of {{Neuromodulation}} in a {{Cortical Network Model}} of {{Object Working Memory Dominated}} by {{Recurrent Inhibition}}},
  author = {Brunel, Nicolas and Wang, Xiao-Jing},
  year = {2001},
  month = jul,
  journal = {J. Comput. Neurosci.},
  volume = {11},
  number = {1},
  pages = {63--85},
  issn = {1573-6873},
  doi = {10.1023/A:1011204814320},
  urldate = {2022-01-17},
  abstract = {Experimental evidence suggests that the maintenance of an item in working memory is achieved through persistent activity in selective neural assemblies of the cortex. To understand the mechanisms underlying this phenomenon, it is essential to investigate how persistent activity is affected by external inputs or neuromodulation. We have addressed these questions using a recurrent network model of object working memory. Recurrence is dominated by inhibition, although persistent activity is generated through recurrent excitation in small subsets of excitatory neurons.},
  langid = {english}
}

@article{buendia_feedback_2020,
  title = {Feedback {{Mechanisms}} for {{Self-Organization}} to the {{Edge}} of a {{Phase Transition}}},
  author = {Buend{\'i}a, Victor and {di Santo}, Serena and Bonachela, Juan A. and Mu{\~n}oz, Miguel A.},
  year = {2020},
  journal = {Front. Phys.},
  volume = {8},
  issn = {2296-424X},
  urldate = {2022-03-25},
  abstract = {Scale-free outbursts of activity are commonly observed in physical, geological, and biological systems. The idea of self-organized criticality (SOC), introduced back in 1987 by Bak, Tang, and Wiesenfeld suggests that, under certain circumstances, natural systems can seemingly self-tune to a critical state with its concomitant power-laws and scaling. Theoretical progress allowed for a rationalization of how SOC works by relating its critical properties to those of a standard non-equilibrium second-order phase transition that separates an active state in which dynamical activity reverberates indefinitely, from an absorbing or quiescent state where activity eventually ceases. The basic mechanism underlying SOC is the alternation of a slow driving process and fast dynamics with dissipation, which generates a feedback loop that tunes the system to the critical point of an absorbing-active continuous phase transition. Here, we briefly review these ideas as well as a recent closely-related concept: self-organized bistability (SOB). In SOB, the very same type of feedback operates in a system characterized by a discontinuous phase transition, which has no critical point but instead presents bistability between active and quiescent states. SOB also leads to scale-invariant avalanches of activity but, in this case, with a different type of scaling and coexisting with anomalously large outbursts. Moreover, SOB explains experiments with real sandpiles more closely than SOC. We review similarities and differences between SOC and SOB by presenting and analyzing them under a common theoretical framework, covering recent results as well as possible future developments. We also discuss other related concepts for ``imperfect'' self-organization such as ``self-organized quasi-criticality'' and ``self-organized collective oscillations,'' of relevance in e.g., neuroscience, with the aim of providing an overview of feedback mechanisms for self-organization to the edge of a phase transition.}
}

@article{buendia_self-organized_2020,
  title = {Self-Organized Bistability and Its Possible Relevance for Brain Dynamics},
  author = {Buend{\'i}a, Victor and {di Santo}, Serena and Villegas, Pablo and Burioni, Raffaella and Mu{\~n}oz, Miguel A.},
  year = {2020},
  month = mar,
  journal = {Phys. Rev. Res.},
  volume = {2},
  number = {1},
  pages = {013318},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevResearch.2.013318},
  urldate = {2022-01-24},
  abstract = {Self-organized bistability (SOB) is the counterpart of ``self-organized criticality'' (SOC), for systems tuning themselves to the edge of bistability of a discontinuous phase transition, rather than to the critical point of a continuous one. The equations defining the mathematical theory of SOB turn out to bear a strong resemblance to a (Landau-Ginzburg) theory recently proposed to analyze the dynamics of the cerebral cortex. This theory describes the neuronal activity of coupled mesoscopic patches of cortex, homeostatically regulated by short-term synaptic plasticity. The theory for cortex dynamics entails, however, some significant differences with respect to SOB, including the lack of a (bulk) conservation law, the absence of a perfect separation of timescales and, the fact that in the former, but not in the second, there is a parameter that controls the overall system state (in blatant contrast with the very idea of self-organization). Here, we scrutinize\textemdash by employing a combination of analytical and computational tools\textemdash the analogies and differences between both theories and explore whether in some limit SOB can play an important role to explain the emergence of scale-invariant neuronal avalanches observed empirically in the cortex. We conclude that, actually, in the limit of infinitely slow synaptic dynamics, the two theories become identical but the timescales required for the self-organization mechanism to be effective do not seem to be biologically plausible. We discuss the key differences between self-organization mechanisms with/without conservation and with/without infinitely separated timescales. In particular, we introduce the concept of ``self-organized collective oscillations'' and scrutinize the implications of our findings in neuroscience, shedding new light into the problems of scale invariance and oscillations in cortical dynamics.}
}

@article{bullerjahn_theory_2014,
  title = {Theory of Rapid Force Spectroscopy},
  author = {Bullerjahn, Jakob T. and Sturm, Sebastian and Kroy, Klaus},
  year = {2014},
  month = dec,
  journal = {Nat. Commun.},
  volume = {5},
  number = {1},
  pages = {4463},
  issn = {2041-1723},
  doi = {10.1038/ncomms5463},
  urldate = {2021-09-17},
  langid = {english}
}

@article{burns_spontaneous_1976,
  title = {The Spontaneous Activity of Neurones in the Cat's Cerebral Cortex},
  author = {Burns, Benedict Delisle and Webb, A. C.},
  year = {1976},
  month = oct,
  journal = {Proc. R. Soc. Lond. B Biol. Sci.},
  volume = {194},
  number = {1115},
  pages = {211--223},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.1976.0074},
  urldate = {2022-02-14},
  abstract = {Chronically implanted microelectrodes have been used to obtain extracellular records of trains of spontaneous action potentials from 30 neurones in the cerebral cortices of 13 unrestrained cats. Recorded neurones were in or near to primary visual cortex, primary auditory cortex, and in the supra-sylvian gyrus. Records were made with animals in several different behavioural states, which included sleep with rapid eye-movements and quiet sleep, peacefully awake, and alarmed. Interval distributions derived from trains of 200 action potentials recorded in less than 80 s did not differ significantly from curves in which the probability of any interval is normally distributed about a modal interval, when plotted on a logarithmic time-axis. Thus the complete interval distributions of neurones firing faster than 2.5/s can be described by two parameters \textendash{} a modal interval, and a geometric standard deviation. This quantitative description of interval distributions proved equally applicable to neurones in all three cortical areas and was valid over the whole range of behavioural states examined. It does not usually hold when the discharge frequency of a neurone is lower than 2.5 action potentials per second. An acceptable fit for a log-normal curve can then only be obtained for intervals that are shorter than about ten times the modal interval. It is pointed out that mean frequency of discharge is a measure of neural activity which is a secondary parameter, since it is dependent upon both modal interval and geometric standard deviation. Our preliminary data show that the two parameters which define the best-fit log-normal curves can vary independently with the behavioural state of the animal in a way that suggests that they may be physiologically important.}
}

@article{can_emergence_2021,
  title = {Emergence of Memory Manifolds},
  author = {Can, Tankut and Krishnamurthy, Kamesh},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.03879},
  eprint = {2109.03879},
  urldate = {2021-09-13},
  abstract = {The ability to store continuous variables in the state of a biological system (e.g. a neural network) is critical for many behaviours. Most models for implementing such a memory manifold require hand-crafted symmetries in the interactions or precise fine-tuning of parameters. We present a general principle that we refer to as \{\textbackslash it frozen stabilisation\}, which allows a family of neural networks to self-organise to a critical state exhibiting memory manifolds without parameter fine-tuning or symmetries. These memory manifolds exhibit a true continuum of memory states and can be used as general purpose integrators for inputs aligned with the manifold. Moreover, frozen stabilisation allows robust memory manifolds in small networks, and this is relevant to debates of implementing continuous attractors with a small number of neurons in light of recent experimental discoveries.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Physics - Biological Physics,Quantitative Biology - Neurons and Cognition}
}

@article{cavanagh_reconciling_2018,
  title = {Reconciling Persistent and Dynamic Hypotheses of Working Memory Coding in Prefrontal Cortex},
  author = {Cavanagh, Sean E. and Towers, John P. and Wallis, Joni D. and Hunt, Laurence T. and Kennerley, Steven W.},
  year = {2018},
  month = aug,
  journal = {Nat. Commun.},
  volume = {9},
  number = {1},
  pages = {3498},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-05873-3},
  urldate = {2022-02-01},
  abstract = {Competing accounts propose that working memory (WM) is subserved either by persistent activity in single neurons or by dynamic (time-varying) activity across a neural population. Here, we compare these hypotheses across four regions of prefrontal cortex (PFC) in an oculomotor-delayed-response task, where an intervening cue indicated the reward available for a correct saccade. WM representations were strongest in ventrolateral PFC neurons with higher intrinsic temporal stability (time-constant). At the population-level, although a stable mnemonic state was reached during the delay, this tuning geometry was reversed relative to cue-period selectivity, and was disrupted by the reward cue. Single-neuron analysis revealed many neurons switched to coding reward, rather than maintaining task-relevant spatial selectivity until saccade. These results imply WM is fulfilled by dynamic, population-level activity within high time-constant neurons. Rather than persistent activity supporting stable mnemonic representations that bridge subsequent salient stimuli, PFC neurons may stabilise a dynamic population-level process supporting WM.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Attention,Network models,Neural decoding,Reward,Working memory}
}

@article{chen_searching_2020-1,
  title = {Searching for Long Time Scales without Fine Tuning},
  author = {Chen, Xiaowen and Bialek, William},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.11674},
  eprint = {2008.11674},
  urldate = {2022-02-22},
  abstract = {Most of animal and human behavior occurs on time scales much longer than the response times of individual neurons. In many cases it is plausible that these long time scales emerge from the recurrent dynamics of electrical activity in networks of neurons. In linear models, time scales are set by the eigenvalues of a dynamical matrix whose elements measure the strengths of synaptic connections between neurons. It is not clear to what extent these matrix elements need to be tuned in order to generate long time scales; in some cases, one needs not just a single long time scale but a whole range. Starting from the simplest case of random symmetric connections, we combine maximum entropy and random matrix theory methods to construct ensembles of networks, exploring the constraints required for long time scales to become generic. We argue that a single long time scale can emerge generically from realistic constraints, but a full spectrum of slow modes requires more tuning. Langevin dynamics that will generate patterns of synaptic connections drawn from these ensembles involve a combination of Hebbian learning and activity-dependent synaptic scaling.},
  archiveprefix = {arxiv},
  langid = {english}
}

@article{chialvo_emergent_2010,
  title = {Emergent Complex Neural Dynamics},
  author = {Chialvo, Dante R.},
  year = {2010},
  month = oct,
  journal = {Nat. Phys.},
  volume = {6},
  number = {10},
  pages = {744--750},
  issn = {1745-2481},
  doi = {10.1038/nphys1803},
  urldate = {2018-07-05},
  abstract = {A large repertoire of spatiotemporal activity patterns in the brain is the basis for adaptive behaviour. Understanding the mechanism by which the brain's hundred billion neurons and hundred trillion synapses manage to produce such a range of cortical configurations in a flexible manner remains a fundamental problem in neuroscience. One plausible solution is the involvement of universal mechanisms of emergent complex phenomena evident in dynamical systems poised near a critical point of a second-order phase transition. We review recent theoretical and empirical results supporting the notion that the brain is naturally poised near criticality, as well as its implications for better understanding of the brain.},
  copyright = {2010 Nature Publishing Group},
  langid = {english}
}

@article{considine_comment_1989,
  title = {Comment on ``{{Noise-induced}} Bistability in a {{Monte Carlo}} Surface-Reaction Model''},
  author = {Considine, D. and Redner, S. and Takayasu, H.},
  year = {1989},
  month = dec,
  journal = {Phys. Rev. Lett.},
  volume = {63},
  number = {26},
  pages = {2857--2857},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.63.2857},
  urldate = {2021-09-17},
  abstract = {A Comment on the Letter by Kristen Fichthorn, Erdogan Gulari, and Robert Ziff, Phys. Rev. Lett. 63, 1527 (1989).}
}

@article{cossart_attractor_2003,
  title = {Attractor Dynamics of Network {{UP}} States in the Neocortex},
  author = {Cossart, Rosa and Aronov, Dmitriy and Yuste, Rafael},
  year = {2003},
  month = may,
  journal = {Nature},
  volume = {423},
  number = {6937},
  pages = {283--288},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature01614},
  urldate = {2022-01-19},
  abstract = {The cerebral cortex receives input from lower brain regions, and its function is traditionally considered to be processing that input through successive stages to reach an appropriate output1,2. However, the cortical circuit contains many interconnections, including those feeding back from higher centres3,4,5,6, and is continuously active even in the absence of sensory inputs7,8,9. Such spontaneous firing has a structure that reflects the coordinated activity of specific groups of neurons10,11,12. Moreover, the membrane potential of cortical neurons fluctuates spontaneously between a resting (DOWN) and a depolarized (UP) state11,13,14,15,16, which may also be coordinated. The elevated firing rate in the UP state follows sensory stimulation16 and provides a substrate for persistent activity, a network state that might mediate working memory17,18,19,20,21. Using two-photon calcium imaging, we reconstructed the dynamics of spontaneous activity of up to 1,400 neurons in slices of mouse visual cortex. Here we report the occurrence of synchronized UP state transitions (`cortical flashes') that occur in spatially organized ensembles involving small numbers of neurons. Because of their stereotyped spatiotemporal dynamics, we conclude that network UP states are circuit attractors\textemdash emergent features of feedback neural networks22 that could implement memory states or solutions to computational problems.},
  copyright = {2003 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research}
}

@article{cramer_control_2020,
  title = {Control of Criticality and Computation in Spiking Neuromorphic Networks with Plasticity},
  author = {Cramer, Benjamin and St{\"o}ckel, David and Kreft, Markus and Wibral, Michael and Schemmel, Johannes and Meier, Karlheinz and Priesemann, Viola},
  year = {2020},
  month = jun,
  journal = {Nat. Commun.},
  volume = {11},
  number = {1},
  pages = {2853},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-16548-3},
  urldate = {2020-10-13},
  abstract = {The critical state is assumed to be optimal for any computation in recurrent neural networks, because criticality maximizes a number of abstract computational properties. We challenge this assumption by evaluating the performance of a spiking recurrent neural network on a set of tasks of varying complexity at - and away from critical network dynamics. To that end, we developed a plastic spiking network on a neuromorphic chip. We show that the distance to criticality can be easily adapted by changing the input strength, and then demonstrate a clear relation between criticality, task-performance and information-theoretic fingerprint. Whereas the information-theoretic measures all show that network capacity is maximal at criticality, only the complex tasks profit from criticality, whereas simple tasks suffer. Thereby, we challenge the general assumption that criticality would be beneficial for any task, and provide instead an understanding of how the collective network state should be tuned to task requirement.},
  copyright = {2020 The Author(s)},
  langid = {english}
}

@article{cramer_heidelberg_2022,
  title = {The {{Heidelberg Spiking Data Sets}} for the {{Systematic Evaluation}} of {{Spiking Neural Networks}}},
  author = {Cramer, Benjamin and Stradmann, Yannik and Schemmel, Johannes and Zenke, Friedemann},
  year = {2022},
  month = jul,
  journal = {IEEE Trans. Neural Netw. Learn. Syst.},
  volume = {33},
  number = {7},
  pages = {2744--2757},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3044364},
  abstract = {Spiking neural networks are the basis of versatile and power-efficient information processing in the brain. Although we currently lack a detailed understanding of how these networks compute, recently developed optimization techniques allow us to instantiate increasingly complex functional spiking neural networks in-silico. These methods hold the promise to build more efficient non-von-Neumann computing hardware and will offer new vistas in the quest of unraveling brain circuit function. To accelerate the development of such methods, objective ways to compare their performance are indispensable. Presently, however, there are no widely accepted means for comparing the computational performance of spiking neural networks. To address this issue, we introduce two spike-based classification data sets, broadly applicable to benchmark both software and neuromorphic hardware implementations of spiking neural networks. To accomplish this, we developed a general audio-to-spiking conversion procedure inspired by neurophysiology. Furthermore, we applied this conversion to an existing and a novel speech data set. The latter is the free, high-fidelity, and word-level aligned Heidelberg digit data set that we created specifically for this study. By training a range of conventional and spiking classifiers, we show that leveraging spike timing information within these data sets is essential for good classification accuracy. These results serve as the first reference for future performance comparisons of spiking neural networks.},
  keywords = {Audio,benchmark,Benchmark testing,Biological neural networks,classification,data set,Encoding,Licenses,neuromorphic computing,spiking neural networks,spoken digits,surrogate gradients,Task analysis,Training,Voltage control}
}

@article{dahmen_global_2022,
  title = {Global Organization of Neuronal Activity Only Requires Unstructured Local Connectivity},
  author = {Dahmen, David and Layer, Moritz and Deutz, Lukas and D{\k{a}}browska, Paulina Anna and Voges, Nicole and {von Papen}, Michael and Brochier, Thomas and Riehle, Alexa and Diesmann, Markus and Gr{\"u}n, Sonja and Helias, Moritz},
  editor = {Palmer, Stephanie E and Behrens, Timothy E},
  year = {2022},
  month = jan,
  journal = {eLife},
  volume = {11},
  pages = {e68422},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.68422},
  urldate = {2022-03-25},
  abstract = {Modern electrophysiological recordings simultaneously capture single-unit spiking activities of hundreds of neurons spread across large cortical distances. Yet, this parallel activity is often confined to relatively low-dimensional manifolds. This implies strong coordination also among neurons that are most likely not even connected. Here, we combine in vivo recordings with network models and theory to characterize the nature of mesoscopic coordination patterns in macaque motor cortex and to expose their origin: We find that heterogeneity in local connectivity supports network states with complex long-range cooperation between neurons that arises from multi-synaptic, short-range connections. Our theory explains the experimentally observed spatial organization of covariances in resting state recordings as well as the behaviorally related modulation of covariance patterns during a reach-to-grasp task. The ubiquity of heterogeneity in local cortical circuits suggests that the brain uses the described mechanism to flexibly adapt neuronal coordination to momentary demands.},
  keywords = {balanced state,beyond-mean-field theory,correlated activity,long-range coordination,motor cortex,reach-to-grasp task}
}

@article{dale_pharmacology_1935,
  title = {Pharmacology and {{Nerve-Endings}}},
  author = {Dale, Henry},
  year = {1935},
  month = jan,
  journal = {Proc. R. Soc. Med.},
  volume = {28},
  number = {3},
  pages = {319--332},
  publisher = {{SAGE Publications}},
  issn = {0035-9157},
  doi = {10.1177/003591573502800330},
  urldate = {2022-02-09},
  abstract = {A brief account is given of the scientific career of Walter Ernest Dixon, and of the importance of his work and his influence for the development of Pharmacology in England. It is suggested that the Memorial Lecture may appropriately deal with some matter of new interest, from one of the fields of research in which Dixon himself was active. Special mention is made of his work with Brodie on the physiology and pharmacology of the bronchioles and the pulmonary blood-vessels, as probably showing the beginning of Dixon's interest in the actions of the alkaloids and organic bases which reproduce the effects of autonomic nerves., An account is given of Dixon's early interest in the suggestion, first made by Elliott, that autonomic nerves transmit their effects by releasing, at their endings, specific substances, which reproduce their actions; and of his attempt to obtain experimental support for this conception. After the War it was established by the experiments of O. Loewi; and it is now generally recognized that parasympathetic effects are so transmitted by release of acetylcholine, sympathetic effects by that of a substance related to adrenaline., Very recent evidence indicates that acetylcholine, by virtue of its other (``nicotine-like'') action, also acts as transmitter of activity at synapses in autonomic ganglia, and from motor nerve to voluntary muscle., The terms ``cholinergic'' and ``adrenergic'' have been introduced to describe nerve-fibres which transmit their actions by the release at their endings of acetylcholine, and of a substance related to adrenaline, respectively. It is shown that Langley and Anderson's evidence, long available, as to the kinds of peripheral efferent fibres which can replace one another in regeneration, can be summarized by the statement, that cholinergic can replace cholinergic fibres, and that adrenergic can replace adrenergic fibres; but that fibres of different chemical function cannot replace one another. The bearing of this new evidence on conceptions of the mode of action of ``neuromimetic'' drugs is discussed. The pharmacological problem can now be more clearly defined, and Dixon's participation in further attempts at its solution will be sadly missed.},
  langid = {english}
}

@article{darshan_canonical_2017,
  title = {A Canonical Neural Mechanism for Behavioral Variability},
  author = {Darshan, Ran and Wood, William E. and Peters, Susan and Leblois, Arthur and Hansel, David},
  year = {2017},
  month = may,
  journal = {Nat. Commun.},
  volume = {8},
  pages = {15415},
  issn = {2041-1723},
  doi = {10.1038/ncomms15415},
  urldate = {2019-01-16},
  abstract = {The ability to generate variable movements is essential for learning and adjusting complex behaviours. This variability has been linked to the temporal irregularity of neuronal activity in the central nervous system. However, how neuronal irregularity actually translates into behavioural variability is unclear. Here we combine modelling, electrophysiological and behavioural studies to address this issue. We demonstrate that a model circuit comprising topographically organized and strongly recurrent neural networks can autonomously generate irregular motor behaviours. Simultaneous recordings of neurons in singing finches reveal that neural correlations increase across the circuit driving song variability, in agreement with the model predictions. Analysing behavioural data, we find remarkable similarities in the babbling statistics of 5\textendash 6-month-old human infants and juveniles from three songbird species and show that our model naturally accounts for these `universal' statistics.},
  copyright = {2017 Nature Publishing Group},
  langid = {english},
  keywords = {spatiotemporal correlations,toread}
}

@article{darshan_strength_2018,
  title = {Strength of {{Correlations}} in {{Strongly Recurrent Neuronal Networks}}},
  author = {Darshan, Ran and {van Vreeswijk}, Carl and Hansel, David},
  year = {2018},
  month = sep,
  journal = {Phys. Rev. X},
  volume = {8},
  number = {3},
  pages = {031072},
  doi = {10.1103/PhysRevX.8.031072},
  urldate = {2019-03-11},
  abstract = {Spatiotemporal correlations in brain activity are functionally important and have been implicated in perception, learning and plasticity, exploratory behavior, and various aspects of cognition. Neurons in the cerebral cortex are strongly interacting. Their activity is temporally irregular and can exhibit substantial correlations. However, how the collective dynamics of highly recurrent and strongly interacting neurons can evolve into a state in which the activity of individual cells is highly irregular yet macroscopically correlated is an open question. Here, we develop a general theory that relates the strength of pairwise correlations to the anatomical features of networks of strongly coupled neurons. To this end, we investigate networks of binary units. When interactions are strong, the activity is irregular in a large region of parameter space. We find that despite the strong interactions, the correlations are generally very weak. Nevertheless, we identify architectural features, which if present, give rise to strong correlations without destroying the irregularity of the activity. For networks with such features, we determine how correlations scale with the network size and the number of connections. Our work shows the mechanism by which strong correlations can be consistent with highly irregular activity, two hallmarks of neuronal dynamics in the central nervous system.}
}

@article{davies_loihi_2018,
  title = {Loihi: {{A Neuromorphic Manycore Processor}} with {{On-Chip Learning}}},
  shorttitle = {Loihi},
  author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  year = {2018},
  month = jan,
  journal = {IEEE Micro},
  volume = {38},
  number = {1},
  pages = {82--99},
  issn = {1937-4143},
  doi = {10.1109/MM.2018.112130359},
  abstract = {Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.},
  keywords = {Algorithm design and analysis,artificial intelligence,Biological neural networks,Computational modeling,Computer architecture,machine learning,neuromorphic computing,Neuromorphics,Neurons}
}

@article{de_heuvel_characterizing_2020,
  title = {Characterizing Spreading Dynamics of Subsampled Systems with Nonstationary External Input},
  author = {{de Heuvel}, Jorge and Wilting, Jens and Becker, Moritz and Priesemann, Viola and Zierenberg, Johannes},
  year = {2020},
  month = oct,
  journal = {Phys. Rev. E},
  volume = {102},
  number = {4},
  pages = {040301},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.102.040301},
  urldate = {2022-01-13},
  abstract = {Many systems with propagation dynamics, such as spike propagation in neural networks and spreading of infectious diseases, can be approximated by autoregressive models. The estimation of model parameters can be complicated by the experimental limitation that one observes only a fraction of the system (subsampling) and potentially time-dependent parameters, leading to incorrect estimates. We show analytically how to overcome the subsampling bias when estimating the propagation rate for systems with certain nonstationary external input. This approach is readily applicable to trial-based experimental setups and seasonal fluctuations as demonstrated on spike recordings from monkey prefrontal cortex and spreading of norovirus and measles.}
}

@article{deneve_efficient_2016,
  title = {Efficient Codes and Balanced Networks},
  author = {Den{\`e}ve, Sophie and Machens, Christian K.},
  year = {2016},
  month = mar,
  journal = {Nat. Neurosci.},
  volume = {19},
  number = {3},
  pages = {375--382},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4243},
  urldate = {2021-06-21},
  abstract = {Despite representing a minority of cortical cells, inhibitory neurons deeply shape cortical responses. Inhibitory currents closely track excitatory currents, opening only brief windows of opportunity for a neuron to fire. This explains the variability of cortical spike trains, but may also, paradoxically, render a spiking network maximally efficient and precise.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Network models;Neural circuits;Neural encoding Subject\_term\_id: network-models;neural-circuit;neural-encoding}
}

@article{destexhe_fluctuating_2001,
  title = {Fluctuating Synaptic Conductances Recreate in Vivo-like Activity in Neocortical Neurons},
  author = {Destexhe, A and Rudolph, M and Fellous, J. -M and Sejnowski, T. J},
  year = {2001},
  month = nov,
  journal = {Neuroscience},
  volume = {107},
  number = {1},
  pages = {13--24},
  issn = {0306-4522},
  doi = {10.1016/S0306-4522(01)00344-X},
  urldate = {2022-02-14},
  abstract = {To investigate the basis of the fluctuating activity present in neocortical neurons in vivo, we have combined computational models with whole-cell recordings using the dynamic-clamp technique. A simplified `point-conductance' model was used to represent the currents generated by thousands of stochastically releasing synapses. Synaptic activity was represented by two independent fast glutamatergic and GABAergic conductances described by stochastic random-walk processes. An advantage of this approach is that all the model parameters can be determined from voltage-clamp experiments. We show that the point-conductance model captures the amplitude and spectral characteristics of the synaptic conductances during background activity. To determine if it can recreate in vivo-like activity, we injected this point-conductance model into a single-compartment model, or in rat prefrontal cortical neurons in vitro using dynamic clamp. This procedure successfully recreated several properties of neurons intracellularly recorded in vivo, such as a depolarized membrane potential, the presence of high-amplitude membrane potential fluctuations, a low-input resistance and irregular spontaneous firing activity. In addition, the point-conductance model could simulate the enhancement of responsiveness due to background activity. We conclude that many of the characteristics of cortical neurons in vivo can be explained by fast glutamatergic and GABAergic conductances varying stochastically.},
  langid = {english},
  keywords = {computational models,CV,dynamic clamp,high-conductance states,pyramidal neurons,synaptic bombardment}
}

@article{destexhe_self-sustained_2009,
  title = {Self-Sustained Asynchronous Irregular States and {{Up}}\textendash{{Down}} States in Thalamic, Cortical and Thalamocortical Networks of Nonlinear Integrate-and-Fire Neurons},
  author = {Destexhe, Alain},
  year = {2009},
  month = jun,
  journal = {J. Comput. Neurosci.},
  volume = {27},
  number = {3},
  pages = {493},
  issn = {1573-6873},
  doi = {10.1007/s10827-009-0164-4},
  urldate = {2021-11-03},
  abstract = {Randomly-connected networks of integrate-and-fire (IF) neurons are known to display asynchronous irregular (AI) activity states, which resemble the discharge activity recorded in the cerebral cortex of awake animals. However, it is not clear whether such activity states are specific to simple IF models, or if they also exist in networks where neurons are endowed with complex intrinsic properties similar to electrophysiological measurements. Here, we investigate the occurrence of AI states in networks of nonlinear IF neurons, such as the adaptive exponential IF (Brette-Gerstner-Izhikevich) model. This model can display intrinsic properties such as low-threshold spike (LTS), regular spiking (RS) or fast-spiking (FS). We successively investigate the oscillatory and AI dynamics of thalamic, cortical and thalamocortical networks using such models. AI states can be found in each case, sometimes with surprisingly small network size of the order of a few tens of neurons. We show that the presence of LTS neurons in cortex or in thalamus, explains the robust emergence of AI states for relatively small network sizes. Finally, we investigate the role of spike-frequency adaptation (SFA). In cortical networks with strong SFA in RS cells, the AI state is transient, but when SFA is reduced, AI states can be self-sustained for long times. In thalamocortical networks, AI states are found when the cortex is itself in an AI state, but with strong SFA, the thalamocortical network displays Up and Down state transitions, similar to intracellular recordings during slow-wave sleep or anesthesia. Self-sustained Up and Down states could also be generated by two-layer cortical networks with LTS cells. These models suggest that intrinsic properties such as adaptation and low-threshold bursting activity are crucial for the genesis and control of AI states in thalamocortical networks.},
  langid = {english}
}

@article{di_carlo_evidence_2022,
  title = {Evidence of Fluctuation-Induced First-Order Phase Transition in Active Matter},
  author = {Di Carlo, Luca and Scandolo, Mattia},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.01010},
  eprint = {2202.01010},
  urldate = {2022-02-14},
  abstract = {We study the critical properties of the Malthusian Toner-Tu theory in its near ordering phase. Because of the birth/death process, characteristic of this Malthusian model, density fluctuations are partially suppressed, and the theory can be studied with perturbative renormalization group techniques. We compute the renormalization group flow equations for the Malthusian Toner-Tu theory at one-loop order. The renormalization group flow drives the system in an unstable region, suggesting a fluctuation-induced first-order phase transition. This calculation could provide a rigorous RG explanation of why the disorder/order transition is first order in active matter systems.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Soft Condensed Matter,Condensed Matter - Statistical Mechanics}
}

@article{di_santo_self-organized_2016,
  title = {Self-{{Organized Bistability Associated}} with {{First-Order Phase Transitions}}},
  author = {{di Santo}, Serena and Burioni, Raffaella and Vezzani, Alessandro and Mu{\~n}oz, Miguel A.},
  year = {2016},
  month = jun,
  journal = {Phys. Rev. Lett.},
  volume = {116},
  number = {24},
  pages = {240601},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.116.240601},
  urldate = {2021-06-21},
  langid = {english}
}

@article{doostmohammadi_onset_2017,
  title = {Onset of Meso-Scale Turbulence in Active Nematics},
  author = {Doostmohammadi, Amin and Shendruk, Tyler N. and Thijssen, Kristian and Yeomans, Julia M.},
  year = {2017},
  month = may,
  journal = {Nat. Commun.},
  volume = {8},
  number = {1},
  pages = {15326},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/ncomms15326},
  urldate = {2022-05-23},
  abstract = {Meso-scale turbulence is an innate phenomenon, distinct from inertial turbulence, that spontaneously occurs at low Reynolds number in fluidized biological systems. This spatiotemporal disordered flow radically changes nutrient and molecular transport in living fluids and can strongly affect the collective behaviour in prominent biological processes, including biofilm formation, morphogenesis and cancer invasion. Despite its crucial role in such physiological processes, understanding meso-scale turbulence and any relation to classical inertial turbulence remains obscure. Here we show how the motion of active matter along a micro-channel transitions to meso-scale turbulence through the evolution of locally disordered patches (active puffs) from an ordered vortex-lattice flow state. We demonstrate that the stationary critical exponents of this transition to meso-scale turbulence in a channel coincide with the directed percolation universality class. This finding bridges our understanding of the onset of low-Reynolds-number meso-scale turbulence and traditional scale-invariant turbulence in confinement.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Biological physics,Fluid dynamics}
}

@article{dornic_integration_2005,
  title = {Integration of {{Langevin Equations}} with {{Multiplicative Noise}} and the {{Viability}} of {{Field Theories}} for {{Absorbing Phase Transitions}}},
  author = {Dornic, Ivan and Chat{\'e}, Hugues and Mu{\~n}oz, Miguel A.},
  year = {2005},
  month = mar,
  journal = {Phys. Rev. Lett.},
  volume = {94},
  number = {10},
  pages = {100601},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.94.100601},
  urldate = {2021-12-07},
  abstract = {Efficient and accurate integration of stochastic (partial) differential equations with multiplicative noise can be obtained through a split-step scheme, which separates the integration of the deterministic part from that of the stochastic part, the latter being performed by sampling exactly the solution of the associated Fokker-Planck equation. We demonstrate the computational power of this method by applying it to the most absorbing phase transitions for which Langevin equations have been proposed. This provides precise estimates of the associated scaling exponents, clarifying the classification of these nonequilibrium problems, and confirms or refutes some existing theories.}
}

@article{douglas_inhibition_2009,
  title = {Inhibition in Cortical Circuits},
  author = {Douglas, Rodney J. and Martin, Kevan A. C.},
  year = {2009},
  month = may,
  journal = {Curr. Biol. CB},
  volume = {19},
  number = {10},
  pages = {R398-402},
  issn = {1879-0445},
  doi = {10.1016/j.cub.2009.03.003},
  langid = {english},
  pmid = {19467204},
  keywords = {Animals,Cerebral Cortex,Nerve Net,Neural Inhibition,Neurons,{Receptors, GABA}}
}

@incollection{eccles_chapter_1986,
  title = {Chapter 1 {{Chemical}} Transmission and {{Dale}}'s Principle},
  booktitle = {Progress in {{Brain Research}}},
  author = {Eccles, John C.},
  editor = {H{\"o}kfelt, T. and Fuxe, K. and Pernow, B.},
  year = {1986},
  month = jan,
  volume = {68},
  pages = {3--13},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(08)60227-7},
  urldate = {2022-02-09},
  abstract = {A tremendous effort has been made to establish the multiplicity of synaptic transmitter substances for a single neuron and its synaptic endings. Ionotropic transmitter action occurs at special receptor sites on the postsynaptic membrane and rapidly opens ionic channels, so that there is a fast and large change in postsynaptic membrane conductance. In contrast, metabotropic transmitter action is effective postsynaptically by stimulating adenylate cyclase to give increased production of cyclic nucleotides with the consequent metabolic change in the postsynaptic cell by the second messenger system. There is overwhelming evidence that the emission of transmitter from the presynaptic terminals is quantal for the classical transmitters and that this quantal emission is due to the exocytosis of synaptic vesicles. The chapter illustrates a simple presynaptic feedback, noradrenalin inhibiting vesicular emission, and the varieties of transmitter interaction. Presynaptic feedbacks are postulated to be negative controls of transmitter release, especially the adenosine triphosphate (ATP) control of the purinergic, noradrenergic, and cholinergic nerve terminals.},
  langid = {english}
}

@article{ehsasi_steady_1989,
  title = {Steady and Nonsteady Rates of Reaction in a Heterogeneously Catalyzed Reaction: {{Oxidation}} of {{CO}} on Platinum, Experiments and Simulations},
  shorttitle = {Steady and Nonsteady Rates of Reaction in a Heterogeneously Catalyzed Reaction},
  author = {Ehsasi, M. and Matloch, M. and Frank, O. and Block, J. H. and Christmann, K. and Rys, F. S. and Hirschwald, W.},
  year = {1989},
  month = oct,
  journal = {J. Chem. Phys.},
  volume = {91},
  number = {8},
  pages = {4949--4960},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.456736},
  urldate = {2022-02-09}
}

@article{ertl_oscillatory_1991,
  title = {Oscillatory {{Kinetics}} and {{Spatio-Temporal Self-Organization}} in {{Reactions}} at {{Solid Surfaces}}},
  author = {Ertl, Gerhard},
  year = {1991},
  month = dec,
  journal = {Science},
  volume = {254},
  number = {5039},
  pages = {1750--1755},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.254.5039.1750},
  urldate = {2022-02-09}
}

@article{feder_homogeneous_1966,
  title = {Homogeneous Nucleation and Growth of Droplets in Vapours},
  author = {Feder, J. and Russell, K.C. and Lothe, J. and Pound, G.M.},
  year = {1966},
  month = jan,
  journal = {Adv. Phys.},
  volume = {15},
  number = {57},
  pages = {111--178},
  publisher = {{Taylor \& Francis}},
  issn = {0001-8732},
  doi = {10.1080/00018736600101264},
  urldate = {2022-01-25},
  abstract = {The theory of homogeneous nucleation of liquid drops in supersaturated vapours is reviewed. A new kinetic treatment which accounts for the heating of the growing clusters due to the latent heat of condensation is presented, and the irreversible thermodynamics of non-isothermal nucleation discussed. It is found that growing clusters are generally colder than the surrounding vapour during their sub-critical growth period. Time dependent nucleation is discussed and a simple estimate for the time-lag in establishing the steady state is given. The nucleation in cloud chambers expanding with constant speed is discussed in detail, and the number of droplets formed and their final mean size is calculated as a function of the terminal expansion ratio. Re-evaporation of nucleated clusters is discussed. Numerical results are given for a number of typical situations in experiments on water-vapour. These results for the nucleation rate as corrected by Lothe and Pound predict much lower `critical' expansion ratios than indicated by the limited experimental data available, even when the non-isothermal theory is used. To restore agreement between theory and experiments the surface tension for small droplets ({$\simeq$}100 molecules) could be increased some 15\% above that of the bulk liquid, or the pre-exponential in the nucleation rate could be decreased by some 10-15. This latter expedient roughly corresponds to use of the `classical' instead of the Lothe-Pound nucleation rate. These two alterations would, however, affect the theoretical curves in different manners and careful experiments could possibly distinguish between them.}
}

@article{feller_two_1951,
  title = {Two {{Singular Diffusion Problems}}},
  author = {Feller, William},
  year = {1951},
  journal = {Ann. Math.},
  volume = {54},
  number = {1},
  eprint = {1969318},
  eprinttype = {jstor},
  pages = {173--182},
  publisher = {{Annals of Mathematics}},
  issn = {0003-486X},
  doi = {10.2307/1969318},
  urldate = {2022-03-11}
}

@article{fichthorn_noise-induced_1989,
  title = {Noise-Induced Bistability in a {{Monte Carlo}} Surface-Reaction Model},
  author = {Fichthorn, Kristen and Gulari, Erdogan and Ziff, Robert},
  year = {1989},
  month = oct,
  journal = {Phys. Rev. Lett.},
  volume = {63},
  number = {14},
  pages = {1527--1530},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.63.1527},
  urldate = {2021-11-05},
  langid = {english}
}

@article{friedmann_demonstrating_2017,
  title = {Demonstrating {{Hybrid Learning}} in a {{Flexible Neuromorphic Hardware System}}},
  author = {Friedmann, Simon and Schemmel, Johannes and Gr{\"u}bl, Andreas and Hartel, Andreas and Hock, Matthias and Meier, Karlheinz},
  year = {2017},
  month = feb,
  journal = {IEEE Trans. Biomed. Circuits Syst.},
  volume = {11},
  number = {1},
  pages = {128--142},
  issn = {1940-9990},
  doi = {10.1109/TBCAS.2016.2579164},
  abstract = {We present results from a new approach to learning and plasticity in neuromorphic hardware systems: to enable flexibility in implementable learning mechanisms while keeping high efficiency associated with neuromorphic implementations, we combine a general-purpose processor with full-custom analog elements. This processor is operating in parallel with a fully parallel neuromorphic system consisting of an array of synapses connected to analog, continuous time neuron circuits. Novel analog correlation sensor circuits process spike events for each synapse in parallel and in real-time. The processor uses this pre-processing to compute new weights possibly using additional information following its program. Therefore, to a certain extent, learning rules can be defined in software giving a large degree of flexibility. Synapses realize correlation detection geared towards Spike-Timing Dependent Plasticity (STDP) as central computational primitive in the analog domain. Operating at a speed-up factor of 1000 compared to biological time-scale, we measure time-constants from tens to hundreds of micro-seconds. We analyze variability across multiple chips and demonstrate learning using a multiplicative STDP rule. We conclude that the presented approach will enable flexible and efficient learning as a platform for neuroscientific research and technological applications.},
  keywords = {Biological system modeling,Correlation,Digital signal processing,Hardware,learning,Mathematical model,neuromorphic hardware,Neuromorphics,Neurons,spike-time dependent plasticity,synapse circuit}
}

@article{furber_large-scale_2016,
  title = {Large-Scale Neuromorphic Computing Systems},
  author = {Furber, Steve},
  year = {2016},
  month = aug,
  journal = {J. Neural Eng.},
  volume = {13},
  number = {5},
  pages = {051001},
  publisher = {{IOP Publishing}},
  issn = {1741-2552},
  doi = {10.1088/1741-2560/13/5/051001},
  urldate = {2021-12-14},
  abstract = {Neuromorphic computing covers a diverse range of approaches to information processing all of which demonstrate some degree of neurobiological inspiration that differentiates them from mainstream conventional computing systems. The philosophy behind neuromorphic computing has its origins in the seminal work carried out by Carver Mead at Caltech in the late 1980s. This early work influenced others to carry developments forward, and advances in VLSI technology supported steady growth in the scale and capability of neuromorphic devices. Recently, a number of large-scale neuromorphic projects have emerged, taking the approach to unprecedented scales and capabilities. These large-scale projects are associated with major new funding initiatives for brain-related research, creating a sense that the time and circumstances are right for progress in our understanding of information processing in the brain. In this review we present a brief history of neuromorphic engineering then focus on some of the principal current large-scale projects, their main features, how their approaches are complementary and distinct, their advantages and drawbacks, and highlight the sorts of capabilities that each can deliver to neural modellers.},
  langid = {english}
}

@article{gallego_neural_2017,
  title = {Neural {{Manifolds}} for the {{Control}} of {{Movement}}},
  author = {Gallego, Juan A. and Perich, Matthew G. and Miller, Lee E. and Solla, Sara A.},
  year = {2017},
  month = jun,
  journal = {Neuron},
  volume = {94},
  number = {5},
  pages = {978--984},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.05.025},
  urldate = {2019-09-26},
  abstract = {The analysis of neural dynamics in several brain cortices has consistently uncovered low-dimensional manifolds that capture a significant fraction of neural variability. These neural manifolds are spanned by specific patterns of correlated neural activity, the ``neural modes.'' We discuss a model for neural control of movement in which the time-dependent activation of these neural modes is the generator of motor behavior. This manifold-based view of motor cortex may lead to a better understanding of how the brain controls movement.}
}

@article{gao_neuronal_2020,
  title = {Neuronal Timescales Are Functionally Dynamic and Shaped by Cortical Microarchitecture},
  author = {Gao, Richard and {van den Brink}, Ruud L and Pfeffer, Thomas and Voytek, Bradley},
  editor = {Vinck, Martin and Colgin, Laura L and Womelsdorf, Thilo},
  year = {2020},
  month = nov,
  journal = {eLife},
  volume = {9},
  pages = {e61277},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.61277},
  urldate = {2021-12-14},
  abstract = {Complex cognitive functions such as working memory and decision-making require information maintenance over seconds to years, from transient sensory stimuli to long-term contextual cues. While theoretical accounts predict the emergence of a corresponding hierarchy of neuronal timescales, direct electrophysiological evidence across the human cortex is lacking. Here, we infer neuronal timescales from invasive intracranial recordings. Timescales increase along the principal sensorimotor-to-association axis across the entire human cortex, and scale with single-unit timescales within macaques. Cortex-wide transcriptomic analysis shows direct alignment between timescales and expression of excitation- and inhibition-related genes, as well as genes specific to voltage-gated transmembrane ion transporters. Finally, neuronal timescales are functionally dynamic: prefrontal cortex timescales expand during working memory maintenance and predict individual performance, while cortex-wide timescales compress with aging. Thus, neuronal timescales follow cytoarchitectonic gradients across the human cortex and are relevant for cognition in both short and long terms, bridging microcircuit physiology with macroscale dynamics and behavior.},
  keywords = {cortical gradients,functional specialization,neuronal timescales,spectral analysis,transcriptomics}
}

@techreport{gao_theory_2017,
  type = {Preprint},
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  year = {2017},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/214262},
  urldate = {2021-10-12},
  abstract = {Abstract           In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  langid = {english}
}

@book{gardiner_handbook_1985,
  title = {Handbook of {{Stochastic Methods}}},
  author = {Gardiner, C W},
  year = {1985},
  publisher = {{Springer}},
  address = {{Berlin}},
  langid = {english}
}

@article{girardi-schappo_synaptic_2020,
  title = {Synaptic Balance Due to Homeostatically Self-Organized Quasicritical Dynamics},
  author = {{Girardi-Schappo}, Mauricio and Brochini, Ludmila and Costa, Ariadne A. and Carvalho, Tawan T. A. and Kinouchi, Osame},
  year = {2020},
  month = feb,
  journal = {Phys. Rev. Res.},
  volume = {2},
  number = {1},
  pages = {012042},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.2.012042},
  urldate = {2021-06-21},
  langid = {english}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  googlebooks = {omivDQAAQBAJ},
  isbn = {978-0-262-33737-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science}
}

@article{goodman_brian_2009,
  title = {The {{Brian}} Simulator},
  author = {Goodman, Dan and Brette, Romain},
  year = {2009},
  journal = {Front. Neurosci.},
  volume = {3},
  pages = {26},
  issn = {1662-453X},
  doi = {10.3389/neuro.01.026.2009},
  urldate = {2021-12-21}
}

@incollection{grotendorst_statistical_2002,
  title = {Statistical {{Analysis}} of {{Sim}} Ulations: {{Data Correlations}} and {{Error Estimation}}},
  booktitle = {Quantum Simulations of Complex Many-Body Systems: From Theory to Algorithms: Winter School, 25 {{February}} - 1 {{March}} 2002, {{Rolduc Conference Centre}}, {{Kerkrade}}, the {{Netherlands}} ; Lecture Notes},
  author = {Janke, Wolfhard},
  editor = {Grotendorst, Johannes},
  year = {2002},
  series = {{{NIC}} Series},
  number = {10},
  publisher = {{NIC}},
  address = {{J\"ulich}},
  isbn = {978-3-00-009057-8},
  langid = {english},
  annotation = {OCLC: 248502198}
}

@article{gu_robust_2021,
  title = {Robust Cortical Criticality and Diverse Dynamics Resulting from Functional Specification},
  author = {Gu, Lei and Wu, Ruqian},
  year = {2021},
  month = apr,
  journal = {Phys. Rev. E},
  volume = {103},
  number = {4},
  pages = {042407},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.103.042407},
  urldate = {2021-04-27},
  langid = {english}
}

@article{harish_asynchronous_2015,
  title = {Asynchronous {{Rate Chaos}} in {{Spiking Neuronal Circuits}}},
  author = {Harish, Omri and Hansel, David},
  year = {2015},
  month = jul,
  journal = {PLOS Comput. Biol.},
  volume = {11},
  number = {7},
  pages = {e1004266},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004266},
  urldate = {2022-12-20},
  abstract = {The brain exhibits temporally complex patterns of activity with features similar to those of chaotic systems. Theoretical studies over the last twenty years have described various computational advantages for such regimes in neuronal systems. Nevertheless, it still remains unclear whether chaos requires specific cellular properties or network architectures, or whether it is a generic property of neuronal circuits. We investigate the dynamics of networks of excitatory-inhibitory (EI) spiking neurons with random sparse connectivity operating in the regime of balance of excitation and inhibition. Combining Dynamical Mean-Field Theory with numerical simulations, we show that chaotic, asynchronous firing rate fluctuations emerge generically for sufficiently strong synapses. Two different mechanisms can lead to these chaotic fluctuations. One mechanism relies on slow I-I inhibition which gives rise to slow subthreshold voltage and rate fluctuations. The decorrelation time of these fluctuations is proportional to the time constant of the inhibition. The second mechanism relies on the recurrent E-I-E feedback loop. It requires slow excitation but the inhibition can be fast. In the corresponding dynamical regime all neurons exhibit rate fluctuations on the time scale of the excitation. Another feature of this regime is that the population-averaged firing rate is substantially smaller in the excitatory population than in the inhibitory population. This is not necessarily the case in the I-I mechanism. Finally, we discuss the neurophysiological and computational significance of our results.},
  langid = {english},
  keywords = {Action potentials,Eigenvalues,Neural networks,Neurons,Simulation and modeling,Soil perturbation,Synapses,Transfer functions}
}

@article{hasson_hierarchical_2015,
  title = {Hierarchical Process Memory: Memory as an Integral Component of Information Processing},
  shorttitle = {Hierarchical Process Memory},
  author = {Hasson, Uri and Chen, Janice and Honey, Christopher J.},
  year = {2015},
  month = jun,
  journal = {Trends Cogn. Sci.},
  volume = {19},
  number = {6},
  pages = {304--313},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2015.04.006},
  urldate = {2021-12-14},
  abstract = {Models of working memory (WM) commonly focus on how information is encoded into and retrieved from storage at specific moments. However, in the majority of real-life processes, past information is used continuously to process incoming information across multiple timescales. Considering single-unit, electrocorticography, and functional imaging data, we argue that (i) virtually all cortical circuits can accumulate information over time, and (ii) the timescales of accumulation vary hierarchically, from early sensory areas with short processing timescales (10s to 100s of milliseconds) to higher-order areas with long processing timescales (many seconds to minutes). In this hierarchical systems perspective, memory is not restricted to a few localized stores, but is intrinsic to information processing that unfolds throughout the brain on multiple timescales.},
  langid = {english}
}

@article{hasson_hierarchical_2015-1,
  title = {Hierarchical Process Memory: Memory as an Integral Component of Information Processing},
  shorttitle = {Hierarchical Process Memory},
  author = {Hasson, Uri and Chen, Janice and Honey, Christopher J.},
  year = {2015},
  month = jun,
  journal = {Trends Cogn. Sci.},
  volume = {19},
  number = {6},
  pages = {304--313},
  issn = {13646613},
  doi = {10.1016/j.tics.2015.04.006},
  urldate = {2019-05-01},
  langid = {english}
}

@article{helmrich_signatures_2020,
  title = {Signatures of Self-Organized Criticality in an Ultracold Atomic Gas},
  author = {Helmrich, S. and Arias, A. and Lochead, G. and Wintermantel, T. M. and Buchhold, M. and Diehl, S. and Whitlock, S.},
  year = {2020},
  month = jan,
  journal = {Nature},
  volume = {577},
  number = {7791},
  pages = {481--486},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1908-6},
  urldate = {2022-08-17},
  abstract = {Self-organized criticality is an elegant explanation of how complex structures emerge and persist throughout nature1, and why such structures often exhibit similar scale-invariant properties2\textendash 9. Although self-organized criticality is sometimes captured by simple models that feature a critical point as an attractor for the dynamics10\textendash 15, the connection to real-world systems is exceptionally hard to test quantitatively16\textendash 21. Here we observe three key signatures of self-organized criticality in the dynamics of a driven\textendash dissipative gas of ultracold potassium atoms: self-organization to a stationary state that is largely independent of the initial conditions; scale-invariance of the final density characterized by a unique scaling function; and large fluctuations of the number of excited atoms (avalanches) obeying a characteristic power-law distribution. This work establishes a well-controlled platform for investigating self-organization phenomena and non-equilibrium criticality, with experimental access to the underlying microscopic details of the system.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Phase transitions and critical phenomena,Ultracold gases}
}

@article{hengen_neuronal_2016,
  title = {Neuronal {{Firing Rate Homeostasis Is Inhibited}} by {{Sleep}} and {{Promoted}} by {{Wake}}},
  author = {Hengen, Keith B. and Torrado Pacheco, Alejandro and McGregor, James N. and Van Hooser, Stephen D. and Turrigiano, Gina G.},
  year = {2016},
  month = mar,
  journal = {Cell},
  volume = {165},
  number = {1},
  pages = {180--191},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2016.01.046},
  urldate = {2021-12-14},
  abstract = {Homeostatic mechanisms stabilize neural circuit function by keeping firing rates within a set-point range, but whether this process is gated by brain state is unknown. Here, we monitored firing rate homeostasis in individual visual cortical neurons in freely behaving rats as they cycled between sleep and wake states. When neuronal firing rates were perturbed by visual deprivation, they gradually returned to a precise, cell-autonomous set point during periods of active wake, with lengthening of the wake period enhancing firing rate rebound. Unexpectedly, this resetting of neuronal firing was suppressed during sleep. This raises the possibility that memory consolidation or other sleep-dependent processes are vulnerable to interference from homeostatic plasticity mechanisms. PaperClip},
  langid = {english}
}

@book{henkel_absorbing_2008,
  title = {Absorbing Phase Transitions},
  author = {Henkel, Malte and Hinrichsen, Haye and L{\"u}beck, Sven},
  year = {2008},
  publisher = {{Springer}},
  address = {{Dordrecht}},
  isbn = {978-1-4020-8764-6 978-1-4020-8765-3},
  langid = {english},
  annotation = {OCLC: 612165422}
}

@article{hidalgo_stochastic_2012,
  title = {Stochastic {{Amplification}} of {{Fluctuations}} in {{Cortical Up-States}}},
  author = {Hidalgo, Jorge and Seoane, Lu{\'i}s F. and Cort{\'e}s, Jes{\'u}s M. and Mu{\~n}oz, Miguel A.},
  year = {2012},
  month = jul,
  journal = {PLOS ONE},
  volume = {7},
  number = {8},
  pages = {e40710},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0040710},
  urldate = {2022-01-28},
  abstract = {Cortical neurons are bistable; as a consequence their local field potentials can fluctuate between quiescent and active states, generating slow Hz oscillations which are widely known as transitions between Up and Down States. Despite a large number of studies on Up-Down transitions, deciphering its nature, mechanisms and function are still today challenging tasks. In this paper we focus on recent experimental evidence, showing that a class of spontaneous oscillations can emerge within the Up states. In particular, a non-trivial peak around Hz appears in their associated power-spectra, what produces an enhancement of the activity power for higher frequencies (in the Hz band). Moreover, this rhythm within Ups seems to be an emergent or collective phenomenon given that individual neurons do not lock to it as they remain mostly unsynchronized. Remarkably, similar oscillations (and the concomitant peak in the spectrum) do not appear in the Down states. Here we shed light on these findings by using different computational models for the dynamics of cortical networks in presence of different levels of physiological complexity. Our conclusion, supported by both theory and simulations, is that the collective phenomenon of ``stochastic amplification of fluctuations'' \textendash{} previously described in other contexts such as Ecology and Epidemiology \textendash{} explains in an elegant and parsimonious manner, beyond model-dependent details, this extra-rhythm emerging only in the Up states but not in the Downs.},
  langid = {english},
  keywords = {Action potentials,Depression,Eigenvalues,Membrane potential,Network analysis,Neural networks,Neurons,Signal amplification}
}

@article{huang_circuit_2019,
  title = {Circuit {{Models}} of {{Low-Dimensional Shared Variability}} in {{Cortical Networks}}},
  author = {Huang, Chengcheng and Ruff, Douglas A. and Pyle, Ryan and Rosenbaum, Robert and Cohen, Marlene R. and Doiron, Brent},
  year = {2019},
  month = jan,
  journal = {Neuron},
  volume = {101},
  number = {2},
  pages = {337-348.e4},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.11.034},
  urldate = {2021-05-10},
  abstract = {Trial-to-trial variability is a reflection of the circuitry and cellular physiology that make up a neuronal network. A pervasive yet puzzling feature of cortical circuits is that despite their complex wiring, population-wide shared spiking variability is low dimensional. Previous model cortical networks cannot explain this global variability, and rather assume it is from external sources. We show that if the spatial and temporal scales of inhibitory coupling match known physiology, networks of model spiking neurons internally generate low-dimensional shared variability that captures population activity recorded in~vivo. Shifting spatial attention into the receptive field of visual neurons has been shown to differentially modulate shared variability within and between brain areas. A top-down modulation of inhibitory neurons in our network provides a parsimonious mechanism for this attentional modulation. Our work provides a critical link between observed cortical circuit structure and realistic shared neuronal variability and its modulation.},
  langid = {english},
  keywords = {attention,cortical model,excitatory/inhibitory balance,low dimensional,neuronal variability,noise correlations}
}

@incollection{ibe_3_2013,
  title = {3 - {{Introduction}} to {{Markov Processes}}},
  booktitle = {Markov {{Processes}} for {{Stochastic Modeling}} ({{Second Edition}})},
  author = {Ibe, Oliver C.},
  editor = {Ibe, Oliver C.},
  year = {2013},
  month = jan,
  pages = {49--57},
  publisher = {{Elsevier}},
  address = {{Oxford}},
  doi = {10.1016/B978-0-12-407795-9.00003-7},
  urldate = {2022-01-12},
  abstract = {A Markov process is a random process in which only the present state influences the next future states. Thus, the distribution of future states depends only on the present state and not how the system arrived at the present state. This chapter presents a general introduction to Markov processes, which are the topics that will be covered in the remainder of the book.},
  isbn = {978-0-12-407795-9},
  langid = {english},
  keywords = {diffusion coefficient,drift coefficient,first passage time,Jump process,Markov chain,Markov property,recurrence time,sojourn time}
}

@article{jaeger_echo_2001,
  title = {The''echo State''approach to Analysing and Training Recurrent Neural Networks},
  author = {Jaeger, H.},
  year = {2001},
  journal = {undefined},
  urldate = {2022-01-13},
  abstract = {The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task. The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task. key words: recurrent neural networks, supervised learning Zusammenfassung. Der Report f\"uhrt ein konstruktives Lernverfahren f\"ur rekurrente neuronale Netze ein, welches zum Erreichen des Lernzieles lediglich die Gewichte der zu den Ausgabeneuronen f\"uhrenden Verbindungen modifiziert. Stichw\"orter: rekurrente neuronale Netze, \"uberwachtes Lernen},
  langid = {english}
}

@article{kale_eigenvalues_nodate,
  title = {Eigenvalues and {{Mixing Time}}},
  author = {Kale, Sagar},
  pages = {9},
  abstract = {Mixing time of a Markov chain depends on the eigenvalues of its transition matrix. We give some examples and bounds on the mixing time in terms of the eigenvalue having second largest absolute value. This paper is based on Chapters 1, 4, and 12 of [1].},
  langid = {english}
}

@book{kampen_stochastic_1992,
  title = {Stochastic {{Processes}} in {{Physics}} and {{Chemistry}}},
  author = {Kampen, N. G. Van},
  year = {1992},
  month = nov,
  publisher = {{Elsevier}},
  abstract = {This new edition of Van Kampen's standard work has been completely revised and updated. Three major changes have also been made. The Langevin equation receives more attention in a separate chapter in which non-Gaussian and colored noise are introduced. Another additional chapter contains old and new material on first-passage times and related subjects which lay the foundation for the chapter on unstable systems. Finally a completely new chapter has been written on the quantum mechanical foundations of noise. The references have also been expanded and updated.},
  googlebooks = {3e7XbMoJzmoC},
  isbn = {978-0-08-057138-6},
  langid = {english},
  keywords = {Science / Chemistry / Physical \& Theoretical,Science / Physics / General}
}

@article{kar_bio_2016,
  title = {Bio Inspired Computing \textendash{} {{A}} Review of Algorithms and Scope of Applications},
  author = {Kar, Arpan Kumar},
  year = {2016},
  month = oct,
  journal = {Expert Syst. Appl.},
  volume = {59},
  pages = {20--32},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2016.04.018},
  urldate = {2021-12-14},
  abstract = {With the explosion of data generation, getting optimal solutions to data driven problems is increasingly becoming a challenge, if not impossible. It is increasingly being recognised that applications of intelligent bio-inspired algorithms are necessary for addressing highly complex problems to provide working solutions in time, especially with dynamic problem definitions, fluctuations in constraints, incomplete or imperfect information and limited computation capacity. More and more such intelligent algorithms are thus being explored for solving different complex problems. While some studies are exploring the application of these algorithms in a novel context, other studies are incrementally improving the algorithm itself. However, the fast growth in the domain makes researchers unaware of the progresses across different approaches and hence awareness across algorithms is increasingly reducing, due to which the literature on bio-inspired computing is skewed towards few algorithms only (like neural networks, genetic algorithms, particle swarm and ant colony optimization). To address this concern, we identify the popularly used algorithms within the domain of bio-inspired algorithms and discuss their principles, developments and scope of application. Specifically, we have discussed the neural networks, genetic algorithm, particle swarm, ant colony optimization, artificial bee colony, bacterial foraging, cuckoo search, firefly, leaping frog, bat algorithm, flower pollination and artificial plant optimization algorithm. Further objectives which could be addressed by these twelve algorithms have also be identified and discussed. This review would pave the path for future studies to choose algorithms based on fitment. We have also identified other bio-inspired algorithms, where there are a lot of scope in theory development and applications, due to the absence of significant literature.},
  langid = {english},
  keywords = {Artificial intelligence,Bio-inspired computing,Intelligent algorithms,Literature review,Metaheuristics,Swarm intelligence}
}

@book{kashchiev_nucleation_2000,
  title = {Nucleation},
  author = {Kashchiev, Dimo},
  year = {2000},
  month = feb,
  publisher = {{Elsevier}},
  abstract = {This book represents a detailed and systematic account of the basic principles, developments and applications of the theory of nucleation.The formation of new phases begins with the process of nucleation and is, therefore, a widely spread phenomenon in both nature and technology. Condensation and evaporation, crystal growth, electrodeposition, melt crystallization, growth of thin films for microelectronics, volcano eruption and formation of particulate matter in space are only a few of the processes in which nucleation plays a prominent role. The book has four parts, which are devoted to the thermodynamics of nucleation, the kinetics of nucleation, the effect of various factors on nucleation and the application of the theory to other processes, which involve nucleation. The first two parts describe in detail the two basic approaches in nucleation theory - the thermodynamic and the kinetic ones. They contain derivations of the basic and most important formulae of the theory and discuss their limitations and possibilities for improvement. The third part deals with some of the factors that can affect nucleation and is a natural continuation of the first two chapters. The last part is devoted to the application of the theory to processes of practical importance such as melt crystallization and polymorphic transformation, crystal growth and growth of thin solid films, size distribution of droplets and crystallites in condensation and crystallization. The book is not just an account of the status quo in nucleation theory - throughout the book there are a number of new results as well as extensions and generalisations of existing ones.},
  isbn = {978-0-08-053783-2},
  langid = {english},
  keywords = {Science / Chemistry / Physical \& Theoretical,Science / Mechanics / Thermodynamics,Science / Physics / Crystallography,Science / Physics / Nuclear,Technology \& Engineering / Chemical \& Biochemical}
}

@article{kim_strong_2021,
  title = {Strong Inhibitory Signaling Underlies Stable Temporal Dynamics and Working Memory in Spiking Neural Networks},
  author = {Kim, Robert and Sejnowski, Terrence J.},
  year = {2021},
  month = jan,
  journal = {Nat. Neurosci.},
  volume = {24},
  number = {1},
  pages = {129--139},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-020-00753-w},
  urldate = {2022-02-01},
  abstract = {Cortical neurons process information on multiple timescales, and areas important for working memory (WM) contain neurons capable of integrating information over a long timescale. However, the underlying mechanisms for the emergence of neuronal timescales stable enough to support WM are unclear. By analyzing a spiking recurrent neural network model trained on a WM task and activity of single neurons in the primate prefrontal cortex, we show that the temporal properties of our model and the neural data are remarkably similar. Dissecting our recurrent neural network model revealed strong inhibitory-to-inhibitory connections underlying a disinhibitory microcircuit as a critical component for long neuronal timescales and WM maintenance. We also found that enhancing inhibitory-to-inhibitory connections led to more stable temporal dynamics and improved task performance. Finally, we show that a network with such microcircuitry can perform other tasks without disrupting its pre-existing timescale architecture, suggesting that strong inhibitory signaling underlies a flexible WM network.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cortex,Network models,Neural circuits,Short-term memory,Working memory}
}

@article{kinouchi_optimal_2006,
  title = {Optimal Dynamical Range of Excitable Networks at Criticality},
  author = {Kinouchi, Osame and Copelli, Mauro},
  year = {2006},
  month = may,
  journal = {Nat. Phys.},
  volume = {2},
  number = {5},
  pages = {348--351},
  publisher = {{Nature Publishing Group}},
  issn = {1745-2481},
  doi = {10.1038/nphys289},
  urldate = {2022-01-03},
  abstract = {A recurrent idea in the study of complex systems is that optimal information processing is to be found near phase transitions. However, this heuristic hypothesis has few (if any) concrete realizations where a standard and biologically relevant quantity is optimized at criticality. Here we give a clear example of such a phenomenon: a network of excitable elements has its sensitivity and dynamic range maximized at the critical point of a non-equilibrium phase transition. Our results are compatible with the essential role of gap junctions in olfactory glomeruli and retinal ganglionar cell output. Synchronization and global oscillations also emerge from the network dynamics. We propose that the main functional role of electrical coupling is to provide an enhancement of dynamic range, therefore allowing the coding of information spanning several orders of magnitude. The mechanism could provide a microscopic neural basis for psychophysical laws.},
  copyright = {2006 Nature Publishing Group},
  langid = {english},
  keywords = {Atomic,Classical and Continuum Physics,Complex Systems,Condensed Matter Physics,general,Mathematical and Computational Physics,Molecular,Optical and Plasma Physics,Physics,Theoretical},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research}
}

@article{kleinert_critical_nodate,
  title = {Critical {{Properties}} of {$\Phi$}4-{{Theories}}},
  author = {Kleinert, Hagen and {Schulte-Frohlinde}, Verena},
  pages = {539},
  langid = {english}
}

@article{kossio_growing_2018,
  title = {Growing {{Critical}}: {{Self-Organized Criticality}} in a {{Developing Neural System}}},
  shorttitle = {Growing {{Critical}}},
  author = {Kossio, Felipe Yaroslav Kalle and Goedeke, Sven and {van den Akker}, Benjamin and Ibarz, Borja and Memmesheimer, Raoul-Martin},
  year = {2018},
  month = aug,
  journal = {Phys. Rev. Lett.},
  volume = {121},
  number = {5},
  pages = {058301},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.121.058301},
  urldate = {2018-08-06},
  langid = {english}
}

@article{landmann_self-organized_2021,
  title = {Self-Organized Criticality in Neural Networks from Activity-Based Rewiring},
  author = {Landmann, Stefan and Baumgarten, Lorenz and Bornholdt, Stefan},
  year = {2021},
  month = mar,
  journal = {Phys. Rev. E},
  volume = {103},
  number = {3},
  pages = {032304},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.103.032304},
  urldate = {2021-03-09},
  langid = {english}
}

@article{latham_correlations_2017,
  title = {Correlations Demystified},
  author = {Latham, Peter E.},
  year = {2017},
  month = jan,
  journal = {Nat. Neurosci.},
  volume = {20},
  number = {1},
  pages = {6--8},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4455},
  urldate = {2022-01-19},
  abstract = {An elegant study answers a long-standing question: how do correlations arise in large, highly interconnected networks of neurons? The answer represents a major step forward in our understanding of spiking networks in the brain.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Network models,Neural circuits},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: News \& Views Subject\_term: Network models;Neural circuits Subject\_term\_id: network-models;neural-circuit}
}

@article{legenstein_edge_2007,
  title = {Edge of Chaos and Prediction of Computational Performance for Neural Circuit Models},
  author = {Legenstein, Robert and Maass, Wolfgang},
  year = {2007},
  month = apr,
  journal = {Neural Netw.},
  volume = {20},
  number = {3},
  pages = {323--334},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.017},
  urldate = {2021-08-23},
  abstract = {We analyze in this article the significance of the edge of chaos for real-time computations in neural microcircuit models consisting of spiking neurons and dynamic synapses. We find that the edge of chaos predicts quite well those values of circuit parameters that yield maximal computational performance. But obviously it makes no prediction of their computational performance for other parameter values. Therefore, we propose a new method for predicting the computational performance of neural microcircuit models. The new measure estimates directly the kernel property and the generalization capability of a neural microcircuit. We validate the proposed measure by comparing its prediction with direct evaluations of the computational performance of various neural microcircuit models. The proposed method also allows us to quantify differences in the computational performance and generalization capability of neural circuits in different dynamic regimes (UP- and DOWN-states) that have been demonstrated through intracellular recordings in vivo.},
  langid = {english}
}

@article{levina_subsampling_2017,
  title = {Subsampling Scaling},
  author = {Levina, A. and Priesemann, V.},
  year = {2017},
  month = may,
  journal = {Nat. Commun.},
  volume = {8},
  number = {1},
  pages = {15140},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/ncomms15140},
  urldate = {2021-10-11},
  abstract = {In real-world applications, observations are often constrained to a small fraction of a system. Such spatial subsampling can be caused by the inaccessibility or the sheer size of the system, and cannot be overcome by longer sampling. Spatial subsampling can strongly bias inferences about a system's aggregated properties. To overcome the bias, we derive analytically a subsampling scaling framework that is applicable to different observables, including distributions of neuronal avalanches, of number of people infected during an epidemic outbreak, and of node degrees. We demonstrate how to infer the correct distributions of the underlying full system, how to apply it to distinguish critical from subcritical systems, and how to disentangle subsampling and finite size effects. Lastly, we apply subsampling scaling to neuronal avalanche models and to recordings from developing neural networks. We show that only mature, but not young networks follow power-law scaling, indicating self-organization to criticality during development.},
  copyright = {2017 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Criticality;Data processing Subject\_term\_id: computational-neuroscience;criticality;data-processing}
}

@article{litwin-kumar_slow_2012,
  title = {Slow Dynamics and High Variability in Balanced Cortical Networks with Clustered Connections},
  author = {{Litwin-Kumar}, Ashok and Doiron, Brent},
  year = {2012},
  month = nov,
  journal = {Nat. Neurosci.},
  volume = {15},
  number = {11},
  pages = {1498--1505},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3220},
  urldate = {2021-07-08},
  abstract = {Excitatory connections in cortex are clustered into groups of highly connected neurons. Here the authors examine the effect this clustering has on the dynamics of neuronal networks with balanced excitation and inhibition. Their model suggests that the reported variability in spontaneous and evoked spiking activity may result from clustered cortical architecture.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Deformation dynamics Subject\_term\_id: deformation-dynamics}
}

@article{ma_cortical_2019,
  title = {Cortical {{Circuit Dynamics Are Homeostatically Tuned}} to {{Criticality In~Vivo}}},
  author = {Ma, Zhengyu and Turrigiano, Gina G. and Wessel, Ralf and Hengen, Keith B.},
  year = {2019},
  month = nov,
  journal = {Neuron},
  volume = {104},
  number = {4},
  pages = {655-664.e4},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.08.031},
  urldate = {2021-12-14},
  abstract = {Homeostatic mechanisms stabilize neuronal activity in~vivo, but whether this process gives rise to balanced network dynamics is unknown. Here, we continuously monitored the statistics of network spiking in visual cortical circuits in freely behaving rats for 9~days. Under control conditions in light and dark, networks were robustly organized around criticality, a regime that maximizes information capacity and transmission. When input was perturbed by visual deprivation, network criticality was severely disrupted and subsequently restored to criticality over 48 h. Unexpectedly, the recovery of excitatory dynamics preceded homeostatic plasticity of firing rates by {$>$}30 h. We utilized model investigations to manipulate firing rate homeostasis in a cell-type-specific manner at the onset of visual deprivation. Our results suggest that criticality in excitatory networks is established by inhibitory plasticity and architecture. These data establish that criticality is consistent with a homeostatic set point for visual cortical dynamics and suggest a key role for homeostatic regulation of inhibition.},
  langid = {english},
  keywords = {computation,cortex,criticality,dynamics,homeostasis,homeostatic plasticity,modeling,visual cortex}
}

@article{maass_real-time_2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  month = nov,
  journal = {Neural Comput.},
  volume = {14},
  number = {11},
  pages = {2531--2560},
  issn = {0899-7667},
  doi = {10.1162/089976602760407955},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.}
}

@article{markovic_physics_2020,
  title = {Physics for Neuromorphic Computing},
  author = {Markovi{\'c}, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
  year = {2020},
  month = sep,
  journal = {Nat. Rev. Phys.},
  volume = {2},
  number = {9},
  pages = {499--510},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5820},
  doi = {10.1038/s42254-020-0208-2},
  urldate = {2021-12-14},
  abstract = {Neuromorphic computing takes inspiration from the brain to create energy-efficient hardware for information processing, capable of highly sophisticated tasks. Systems built with standard electronics achieve gains in speed and energy by mimicking the distributed topology of the brain. Scaling-up such systems and improving their energy usage, speed and performance by several orders of magnitude requires a revolution in hardware. We discuss how including more physics in the algorithms and nanoscale materials used for data processing could have a major impact in the field of neuromorphic computing. We review striking results that leverage physics to enhance the computing capabilities of artificial neural networks, using resistive switching materials, photonics, spintronics and other technologies. We discuss the paths that could lead these approaches to maturity, towards low-power, miniaturized chips that could infer and learn in real time.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Electronics,Nanoscale devices,photonics and device physics},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Electronics, photonics and device physics;Nanoscale devices Subject\_term\_id: electronics-photonics-and-device-physics;nanoscale-devices}
}

@article{marriott_bias_1954,
  title = {Bias in the {{Estimation}} of {{Autocorrelations}}},
  author = {Marriott, F. H. C. and Pope, J. A.},
  year = {1954},
  journal = {Biometrika},
  volume = {41},
  number = {3/4},
  eprint = {2332719},
  eprinttype = {jstor},
  pages = {390--402},
  issn = {0006-3444},
  doi = {10.2307/2332719},
  urldate = {2019-10-11}
}

@article{martin_eluding_2015,
  title = {Eluding Catastrophic Shifts},
  author = {Mart{\'i}n, Paula Villa and Bonachela, Juan A. and Levin, Simon A. and Mu{\~n}oz, Miguel A.},
  year = {2015},
  month = apr,
  journal = {Proc. Natl. Acad. Sci.},
  volume = {112},
  number = {15},
  pages = {E1828-E1836},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1414708112},
  urldate = {2021-09-13},
  abstract = {Transitions between regimes with radically different properties are ubiquitous in nature. Such transitions can occur either smoothly or in an abrupt and catastrophic fashion. Important examples of the latter can be found in ecology, climate sciences, and economics, to name a few, where regime shifts have catastrophic consequences that are mostly irreversible (e.g., desertification, coral reef collapses, and market crashes). Predicting and preventing these abrupt transitions remains a challenging and important task. Usually, simple deterministic equations are used to model and rationalize these complex situations. However, stochastic effects might have a profound effect. Here we use 1D and 2D spatially explicit models to show that intrinsic (demographic) stochasticity can alter deterministic predictions dramatically, especially in the presence of other realistic features such as limited mobility or spatial heterogeneity. In particular, these ingredients can alter the possibility of catastrophic shifts by giving rise to much smoother and easily reversible continuous ones. The ideas presented here can help further understand catastrophic shifts and contribute to the discussion about the possibility of preventing such shifts to minimize their disruptive ecological, economic, and societal consequences.},
  chapter = {PNAS Plus},
  langid = {english},
  pmid = {25825772},
  keywords = {catastrophic shifts,critical transitions,demographic stochasticity,nonequilibrium phase transitions,renormalization group}
}

@article{martin_fluctuation-induced_2021,
  title = {Fluctuation-{{Induced Phase Separation}} in {{Metric}} and {{Topological Models}} of {{Collective Motion}}},
  author = {Martin, David and Chat{\'e}, Hugues and Nardini, Cesare and Solon, Alexandre and Tailleur, Julien and Van Wijland, Fr{\'e}d{\'e}ric},
  year = {2021},
  month = apr,
  journal = {Phys. Rev. Lett.},
  volume = {126},
  number = {14},
  pages = {148001},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.126.148001},
  urldate = {2022-02-14},
  abstract = {We study the role of noise on the nature of the transition to collective motion in dry active matter. Starting from field theories that predict a continuous transition at the deterministic level, we show that fluctuations induce a density-dependent shift of the onset of order, which in turn changes the nature of the transition into a phase-separation scenario. Our results apply to a range of systems, including models in which particles interact with their ``topological'' neighbors that have been believed so far to exhibit a continuous onset of order. Our analytical predictions are confirmed by numerical simulations of fluctuating hydrodynamics and microscopic models.}
}

@article{mastrogiuseppe_intrinsically-generated_2017,
  title = {Intrinsically-Generated Fluctuating Activity in Excitatory-Inhibitory Networks},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2017},
  month = apr,
  journal = {PLOS Comput. Biol.},
  volume = {13},
  number = {4},
  pages = {e1005498},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005498},
  urldate = {2021-12-14},
  abstract = {Recurrent networks of non-linear units display a variety of dynamical regimes depending on the structure of their synaptic connectivity. A particularly remarkable phenomenon is the appearance of strongly fluctuating, chaotic activity in networks of deterministic, but randomly connected rate units. How this type of intrinsically generated fluctuations appears in more realistic networks of spiking neurons has been a long standing question. To ease the comparison between rate and spiking networks, recent works investigated the dynamical regimes of randomly-connected rate networks with segregated excitatory and inhibitory populations, and firing rates constrained to be positive. These works derived general dynamical mean field (DMF) equations describing the fluctuating dynamics, but solved these equations only in the case of purely inhibitory networks. Using a simplified excitatory-inhibitory architecture in which DMF equations are more easily tractable, here we show that the presence of excitation qualitatively modifies the fluctuating activity compared to purely inhibitory networks. In presence of excitation, intrinsically generated fluctuations induce a strong increase in mean firing rates, a phenomenon that is much weaker in purely inhibitory networks. Excitation moreover induces two different fluctuating regimes: for moderate overall coupling, recurrent inhibition is sufficient to stabilize fluctuations; for strong coupling, firing rates are stabilized solely by the upper bound imposed on activity, even if inhibition is stronger than excitation. These results extend to more general network architectures, and to rate networks receiving noisy inputs mimicking spiking activity. Finally, we show that signatures of the second dynamical regime appear in networks of integrate-and-fire neurons.},
  langid = {english},
  keywords = {Action potentials,Autocorrelation,Dynamical systems,Eigenvalues,Neural networks,Neurons,Transfer functions,White noise}
}

@article{mastrogiuseppe_intrinsically-generated_2017-1,
  title = {Intrinsically-Generated Fluctuating Activity in Excitatory-Inhibitory Networks},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2017},
  month = apr,
  journal = {PLOS Comput. Biol.},
  volume = {13},
  number = {4},
  pages = {e1005498},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005498},
  urldate = {2022-01-13},
  abstract = {Recurrent networks of non-linear units display a variety of dynamical regimes depending on the structure of their synaptic connectivity. A particularly remarkable phenomenon is the appearance of strongly fluctuating, chaotic activity in networks of deterministic, but randomly connected rate units. How this type of intrinsically generated fluctuations appears in more realistic networks of spiking neurons has been a long standing question. To ease the comparison between rate and spiking networks, recent works investigated the dynamical regimes of randomly-connected rate networks with segregated excitatory and inhibitory populations, and firing rates constrained to be positive. These works derived general dynamical mean field (DMF) equations describing the fluctuating dynamics, but solved these equations only in the case of purely inhibitory networks. Using a simplified excitatory-inhibitory architecture in which DMF equations are more easily tractable, here we show that the presence of excitation qualitatively modifies the fluctuating activity compared to purely inhibitory networks. In presence of excitation, intrinsically generated fluctuations induce a strong increase in mean firing rates, a phenomenon that is much weaker in purely inhibitory networks. Excitation moreover induces two different fluctuating regimes: for moderate overall coupling, recurrent inhibition is sufficient to stabilize fluctuations; for strong coupling, firing rates are stabilized solely by the upper bound imposed on activity, even if inhibition is stronger than excitation. These results extend to more general network architectures, and to rate networks receiving noisy inputs mimicking spiking activity. Finally, we show that signatures of the second dynamical regime appear in networks of integrate-and-fire neurons.},
  langid = {english},
  keywords = {Action potentials,Autocorrelation,Dynamical systems,Eigenvalues,Neural networks,Neurons,Transfer functions,White noise}
}

@article{may_ecology_2008,
  title = {Ecology for Bankers},
  author = {May, Robert M. and Levin, Simon A. and Sugihara, George},
  year = {2008},
  month = feb,
  journal = {Nature},
  volume = {451},
  number = {7181},
  pages = {893--894},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/451893a},
  urldate = {2022-02-14},
  abstract = {There is common ground in analysing financial systems and ecosystems, especially in the need to identify conditions that dispose a system to be knocked from seeming stability into another, less happy state.},
  copyright = {2008 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@article{mazzucato_dynamics_2015,
  title = {Dynamics of {{Multistable States}} during {{Ongoing}} and {{Evoked Cortical Activity}}},
  author = {Mazzucato, L. and Fontanini, A. and La Camera, G.},
  year = {2015},
  month = may,
  journal = {J. Neurosci.},
  volume = {35},
  number = {21},
  pages = {8214--8231},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4819-14.2015},
  urldate = {2021-06-08},
  langid = {english}
}

@article{mazzucato_stimuli_2016,
  title = {Stimuli {{Reduce}} the {{Dimensionality}} of {{Cortical Activity}}},
  author = {Mazzucato, Luca and Fontanini, Alfredo and La Camera, Giancarlo},
  year = {2016},
  journal = {Front. Syst. Neurosci.},
  volume = {10},
  publisher = {{Frontiers}},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2016.00011},
  urldate = {2021-06-08},
  abstract = {The activity of ensembles of simultaneously recorded neurons can be represented as a set of points in the space of firing rates. Even though the dimension of this space is equal to the ensemble size, neural activity can be effectively localized on smaller subspaces. The dimensionality of the neural space is an important determinant of the computational tasks supported by the neural activity. Here, we investigate the dimensionality of neural ensembles from the sensory cortex of alert rats during periods of ongoing (inter-trial) and stimulus-evoked activity. We find that dimensionality grows linearly with ensemble size, and grows significantly faster during ongoing activity compared to evoked activity. We explain these results using a spiking network model based on a clustered architecture. The model captures the difference in growth rate between ongoing and evoked activity and predicts a characteristic scaling with ensemble size that could be tested in high-density multi-electrode recordings. Moreover, we present a simple theory that predicts the existence of an upper bound on dimensionality. This upper bound is inversely proportional to the amount of pair-wise correlations and, compared to a homogeneous network without clusters, it is larger by a factor equal to the number of clusters. The empirical estimation of such bounds depends on the number and duration of trials and is well predicted by the theory. Together, these results provide a framework to analyze neural dimensionality in alert animals, its behavior under stimulus presentation, and its theoretical dependence on ensemble size, number of clusters, and correlations in spiking network models.},
  langid = {english},
  keywords = {dimensionality,gustatory cortex,Hidden Markov Models,mean field theory,Metastable dynamics,ongoing activity,spiking network model}
}

@article{mcdonnell_benefits_2011,
  title = {The Benefits of Noise in Neural Systems: Bridging Theory and Experiment},
  shorttitle = {The Benefits of Noise in Neural Systems},
  author = {McDonnell, Mark D. and Ward, Lawrence M.},
  year = {2011},
  month = jul,
  journal = {Nat. Rev. Neurosci.},
  volume = {12},
  number = {7},
  pages = {415--425},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3061},
  urldate = {2020-10-13},
  abstract = {Both theoretical and experimental approaches have demonstrated that noise can improve information processing, but there is substantial scope for new biologically appropriate computational hypotheses and noise sources to be investigated. McDonnell and Ward propose a unifying framework for reconciling theory with experiment.},
  copyright = {2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english}
}

@article{menesse_homeostatic_2021,
  title = {Homeostatic {{Criticality}} in {{Neuronal Networks}}},
  author = {Menesse, Eduardo and Marin, B{\'o}ris and Kinouchi, Osame},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.02718},
  eprint = {2109.02718},
  urldate = {2021-09-09},
  abstract = {In self-organized criticality (SOC) models, as well as in standard phase transitions, criticality is only present for vanishing driving external fields \$h \textbackslash rightarrow 0\$. Considering that this is rarely the case for natural systems, such a restriction poses a challenge to the explanatory power of these models. Besides that, in models of dissipative systems like earthquakes, forest fires and neuronal networks, there is no true critical behavior, as expressed in clean power laws obeying finite-size scaling, but a scenario called "dirty" criticality or self-organized quasi-criticality (SOqC). Here, we propose simple homeostatic mechanisms which promote self-organization of coupling strengths, gains, and firing thresholds in neuronal networks. We show that near criticality can be reached and sustained even in the presence of external inputs because the firing thresholds adapt to and cancel the inputs, a phenomenon similar to perfect adaptation in sensory systems. Similar mechanisms can be proposed for the couplings and local thresholds in spin systems and cellular automata, which could lead to applications in earthquake, forest fire, stellar flare, voting and epidemic modeling.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Neurons and Cognition}
}

@article{millman_self-organized_2010,
  title = {Self-Organized Criticality Occurs in Non-Conservative Neuronal Networks during `up' States},
  author = {Millman, Daniel and Mihalas, Stefan and Kirkwood, Alfredo and Niebur, Ernst},
  year = {2010},
  month = oct,
  journal = {Nat. Phys.},
  volume = {6},
  number = {10},
  pages = {801--805},
  issn = {1745-2481},
  doi = {10.1038/nphys1757},
  urldate = {2018-10-16},
  abstract = {During sleep, under anaesthesia and in vitro, cortical neurons in sensory, motor, association and executive areas fluctuate between so-called up and down states, which are characterized by distinct membrane potentials and spike rates1,2,3,4,5. Another phenomenon observed in preparations similar to those that exhibit up and down states\textemdash such as anaesthetized rats6, brain slices and cultures devoid of sensory input7, as well as awake monkey cortex8\textemdash is self-organized criticality (SOC). SOC is characterized by activity `avalanches' with a branching parameter near unity and size distribution that obeys a power law with a critical exponent of about -3/2. Recent work has demonstrated SOC in conservative neuronal network models9,10, but critical behaviour breaks down when biologically realistic `leaky' neurons are introduced9. Here, we report robust SOC behaviour in networks of non-conservative leaky integrate-and-fire neurons with short-term synaptic depression. We show analytically and numerically that these networks typically have two stable activity levels, corresponding to up and down states, that the networks switch spontaneously between these states and that up states are critical and down states are subcritical.},
  copyright = {2010 Nature Publishing Group},
  langid = {english}
}

@article{monroe_neuromorphic_2014,
  title = {Neuromorphic Computing Gets Ready for the (Really) Big Time},
  author = {Monroe, Don},
  year = {2014},
  month = jun,
  journal = {Commun. ACM},
  volume = {57},
  number = {6},
  pages = {13--15},
  issn = {0001-0782},
  doi = {10.1145/2601069},
  urldate = {2021-12-14},
  abstract = {A technology inspired by biological principles but 'steamrolled for decades' prepares to take off as Moore's Law approaches its long-anticipated end.}
}

@article{moradi_scalable_2018,
  title = {A {{Scalable Multicore Architecture With Heterogeneous Memory Structures}} for {{Dynamic Neuromorphic Asynchronous Processors}} ({{DYNAPs}})},
  author = {Moradi, Saber and Qiao, Ning and Stefanini, Fabio and Indiveri, Giacomo},
  year = {2018},
  month = feb,
  journal = {IEEE Trans. Biomed. Circuits Syst.},
  volume = {12},
  number = {1},
  pages = {106--122},
  issn = {1940-9990},
  doi = {10.1109/TBCAS.2017.2759700},
  abstract = {Neuromorphic computing systems comprise networks of neurons that use asynchronous events for both computation and communication. This type of representation offers several advantages in terms of bandwidth and power consumption in neuromorphic electronic systems. However, managing the traffic of asynchronous events in large scale systems is a daunting task, both in terms of circuit complexity and memory requirements. Here, we present a novel routing methodology that employs both hierarchical and mesh routing strategies and combines heterogeneous memory structures for minimizing both memory requirements and latency, while maximizing programming flexibility to support a wide range of event-based neural network architectures, through parameter configuration. We validated the proposed scheme in a prototype multicore neuromorphic processor chip that employs hybrid analog/digital circuits for emulating synapse and neuron dynamics together with asynchronous digital circuits for managing the address-event traffic. We present a theoretical analysis of the proposed connectivity scheme, describe the methods and circuits used to implement such scheme, and characterize the prototype chip. Finally, we demonstrate the use of the neuromorphic processor with a convolutional neural network for the real-time classification of visual symbols being flashed to a dynamic vision sensor (DVS) at high speed.},
  keywords = {Asynchronous,circuits and systems,Memory management,neuromorphic computing,Neuromorphics,Neurons,Routing,routing architectures}
}

@article{moreno-bote_information-limiting_2014,
  title = {Information-Limiting Correlations},
  author = {{Moreno-Bote}, Rub{\'e}n and Beck, Jeffrey and Kanitscheider, Ingmar and Pitkow, Xaq and Latham, Peter and Pouget, Alexandre},
  year = {2014},
  month = oct,
  journal = {Nat. Neurosci.},
  volume = {17},
  number = {10},
  pages = {1410--1417},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3807},
  urldate = {2022-01-19},
  abstract = {Correlations of noise in neural population activity are thought to limit the amount of information contained in such population activity, whereas decorrelation is suggested to increase information content. Here the authors show that decorrelation does not imply an increase in information, and only certain types of correlations limit information content.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Neural encoding},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Neural encoding Subject\_term\_id: neural-encoding}
}

@article{moreno-bote_noise-induced_2007,
  title = {Noise-{{Induced Alternations}} in an {{Attractor Network Model}} of {{Perceptual Bistability}}},
  author = {{Moreno-Bote}, Rub{\'e}n and Rinzel, John and Rubin, Nava},
  year = {2007},
  month = sep,
  journal = {J. Neurophysiol.},
  volume = {98},
  number = {3},
  pages = {1125--1139},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00116.2007},
  urldate = {2021-09-17},
  abstract = {When a stimulus supports two distinct interpretations, perception alternates in an irregular manner between them. What causes the bistable perceptual switches remains an open question. Most existing models assume that switches arise from a slow fatiguing process, such as adaptation or synaptic depression. We develop a new, attractor-based framework in which alternations are induced by noise and are absent without it. Our model goes beyond previous energy-based conceptualizations of perceptual bistability by constructing a neurally plausible attractor model that is implemented in both firing rate mean-field and spiking cell-based networks. The model accounts for known properties of bistable perceptual phenomena, most notably the increase in alternation rate with stimulation strength observed in binocular rivalry. Furthermore, it makes a novel prediction about the effect of changing stimulus strength on the activity levels of the dominant and suppressed neural populations, a prediction that could be tested with functional MRI or electrophysiological recordings. The neural architecture derived from the energy-based model readily generalizes to several competing populations, providing a natural extension for multistability phenomena.}
}

@article{muller_diffuse_2020,
  title = {Diffuse Neural Coupling Mediates Complex Network Dynamics through the Formation of Quasi-Critical Brain States},
  author = {M{\"u}ller, Eli J. and Munn, Brandon R. and Shine, James M.},
  year = {2020},
  month = dec,
  journal = {Nat. Commun.},
  volume = {11},
  number = {1},
  pages = {6337},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19716-7},
  urldate = {2021-11-10},
  abstract = {The biological mechanisms that allow the brain to balance flexibility and integration remain poorly understood. A potential solution may lie in a unique aspect of neurobiology, which is that numerous brain systems contain diffuse synaptic connectivity. Here, we demonstrate that increasing diffuse cortical coupling within a validated biophysical corticothalamic model traverses the system through a quasi-critical regime in which spatial heterogeneities in input noise support transient critical dynamics in distributed subregions. The presence of quasi-critical states coincides with known signatures of complex, adaptive brain network dynamics. Finally, we demonstrate the presence of similar dynamic signatures in empirical whole-brain human neuroimaging data. Together, our results establish that modulating the balance between local and diffuse synaptic coupling in a thalamocortical model subtends the emergence of quasi-critical brain states that act to flexibly transition the brain between unique modes of information processing.},
  copyright = {2020 Crown},
  langid = {english},
  keywords = {Biophysical models,Dynamical systems},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Biophysical models;Dynamical systems Subject\_term\_id: biophysical-models;dynamical-systems}
}

@article{muller_extending_2020,
  title = {Extending {{BrainScaleS OS}} for {{BrainScaleS-2}}},
  author = {M{\"u}ller, Eric and Mauch, Christian and Spilger, Philipp and Breitwieser, Oliver Julien and Kl{\"a}hn, Johann and St{\"o}ckel, David and Wunderlich, Timo and Schemmel, Johannes},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.13750},
  eprint = {2003.13750},
  urldate = {2022-04-26},
  abstract = {BrainScaleS-2 is a mixed-signal accelerated neuromorphic system targeted for research in the fields of computational neuroscience and beyond-von-Neumann computing. To augment its flexibility, the analog neural network core is accompanied by an embedded SIMD microprocessor. The BrainScaleS Operating System (BrainScaleS OS) is a software stack designed for the user-friendly operation of the BrainScaleS architectures. We present and walk through the software-architectural enhancements that were introduced for the BrainScaleS-2 architecture. Finally, using a second-version BrainScaleS-2 prototype we demonstrate its application in an example experiment based on spike-based expectation maximization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Neural and Evolutionary Computing}
}

@article{muller_scalable_2022,
  title = {A {{Scalable Approach}} to {{Modeling}} on {{Accelerated Neuromorphic Hardware}}},
  author = {M{\"u}ller, Eric and Arnold, Elias and Breitwieser, Oliver and Czierlinski, Milena and Emmel, Arne and Kaiser, Jakob and Mauch, Christian and Schmitt, Sebastian and Spilger, Philipp and Stock, Raphael and Stradmann, Yannik and Weis, Johannes and Baumbach, Andreas and Billaudelle, Sebastian and Cramer, Benjamin and Ebert, Falk and G{\"o}ltz, Julian and Ilmberger, Joscha and Karasenko, Vitali and Kleider, Mitja and Leibfried, Aron and Pehle, Christian and Schemmel, Johannes},
  year = {2022},
  journal = {Front. Neurosci.},
  volume = {16},
  issn = {1662-453X},
  urldate = {2022-07-18},
  abstract = {Neuromorphic systems open up opportunities to enlarge the explorative space for computational research. However, it is often challenging to unite efficiency and usability. This work presents the software aspects of this endeavor for the BrainScaleS-2 system, a hybrid accelerated neuromorphic hardware architecture based on physical modeling. We introduce key aspects of the BrainScaleS-2 Operating System: experiment workflow, API layering, software design, and platform operation. We present use cases to discuss and derive requirements for the software and showcase the implementation. The focus lies on novel system and software features such as multi-compartmental neurons, fast re-configuration for hardware-in-the-loop training, applications for the embedded processors, the non-spiking operation mode, interactive platform access, and sustainable hardware/software co-development. Finally, we discuss further developments in terms of hardware scale-up, system usability, and efficiency.}
}

@article{munoz_colloquium_2018,
  title = {Colloquium: {{Criticality}} and Dynamical Scaling in Living Systems},
  shorttitle = {Colloquium},
  author = {Mu{\~n}oz, Miguel A.},
  year = {2018},
  month = jul,
  journal = {Rev. Mod. Phys.},
  volume = {90},
  number = {3},
  pages = {031001},
  doi = {10.1103/RevModPhys.90.031001},
  urldate = {2018-07-30},
  abstract = {A celebrated and controversial hypothesis suggests that some biological systems\textemdash parts, aspects, or groups of them\textemdash may extract important functional benefits from operating at the edge of instability, halfway between order and disorder, i.e., in the vicinity of the critical point of a phase transition. Criticality has been argued to provide biological systems with an optimal balance between robustness against perturbations and flexibility to adapt to changing conditions as well as to confer on them optimal computational capabilities, large dynamical repertoires, unparalleled sensitivity to stimuli, etc. Criticality, with its concomitant scale invariance, can be conjectured to emerge in living systems as the result of adaptive and evolutionary processes that, for reasons to be fully elucidated, select for it as a template upon which further layers of complexity can rest. This hypothesis is suggestive as it proposes that criticality could constitute a general and common organizing strategy in biology stemming from the physics of phase transitions. However, despite its implications, this is still in its infancy state as a well-founded theory and, as such, it has elicited some skepticism. From the experimental side, the advent of high-throughput technologies has created new prospects in the exploration of biological systems, and empirical evidence in favor of criticality has proliferated, with examples ranging from endogenous brain activity and gene-expression patterns to flocks of birds and insect-colony foraging, to name but a few. Some pieces of evidence are quite remarkable, while in some other cases empirical data are limited, incomplete, or not fully convincing. More stringent experimental setups and theoretical analyses are certainly needed to fully clarify the picture. In any case, the time seems ripe for bridging the gap between this theoretical conjecture and its empirical validation. Given the profound implications of shedding light on this issue, it is both pertinent and timely to review the state of the art and to discuss future strategies and perspectives.}
}

@article{munoz_nature_1998,
  title = {Nature of Different Types of Absorbing States},
  author = {Mu{\~n}oz, Miguel A.},
  year = {1998},
  month = feb,
  journal = {Phys. Rev. E},
  volume = {57},
  number = {2},
  pages = {1377--1383},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.57.1377},
  urldate = {2022-02-22},
  abstract = {We present a comparison of three different types of Langevin equations exhibiting absorbing states: the Langevin equation defining the Reggeon field theory, one with multiplicative noise, and a third type in which the noise is complex. Each one is found to describe a different underlying physical mechanism; in particular, the nature of the different absorbing states depends on the type of noise considered. By studying the stationary single-site effective potential, we analyze the impossibility of finding a reaction-diffusion model in the multiplicative noise universality class. We also discuss some theoretical questions related to the nature of complex noise, as for example, whether it is necessary or not to consider a complex equation in order to describe processes as the annihilation reaction, A+\textrightarrow A0.}
}

@article{murray_hierarchy_2014,
  title = {A Hierarchy of Intrinsic Timescales across Primate Cortex},
  author = {Murray, John D. and Bernacchia, Alberto and Freedman, David J. and Romo, Ranulfo and Wallis, Jonathan D. and Cai, Xinying and {Padoa-Schioppa}, Camillo and Pasternak, Tatiana and Seo, Hyojung and Lee, Daeyeol and Wang, Xiao-Jing},
  year = {2014},
  month = dec,
  journal = {Nat. Neurosci.},
  volume = {17},
  number = {12},
  pages = {1661--1663},
  issn = {1546-1726},
  doi = {10.1038/nn.3862},
  urldate = {2019-02-19},
  abstract = {Specialization and hierarchy are organizing principles for primate cortex, yet there is little direct evidence for how cortical areas are specialized in the temporal domain. We measured timescales of intrinsic fluctuations in spiking activity across areas and found a hierarchical ordering, with sensory and prefrontal areas exhibiting shorter and longer timescales, respectively. On the basis of our findings, we suggest that intrinsic timescales reflect areal specialization for task-relevant computations over multiple temporal ranges.},
  copyright = {2014 Nature Publishing Group},
  langid = {english}
}

@inproceedings{neftci_device_2010,
  title = {A Device Mismatch Compensation Method for {{VLSI}} Neural Networks},
  booktitle = {2010 {{Biomed}}. {{Circuits Syst}}. {{Conf}}. {{BioCAS}}},
  author = {Neftci, Emre and Indiveri, Giacomo},
  year = {2010},
  month = nov,
  pages = {262--265},
  issn = {2163-4025},
  doi = {10.1109/BIOCAS.2010.5709621},
  abstract = {Device mismatch in neuromorphic VLSI implementations of spiking neural networks can be a serious and limiting problem. Classical engineering solutions can reduce the effect of mismatch, but require increasing layout sizes or using additional precious silicon real-estate. Here we propose a complementary strategy which exploits the Address-Event Representation used in neuromorphic systems and does not affect the device layout. We propose a method that selectively changes the connectivity profile in the neural network to normalize its response. We provide a theoretical analysis of the approach proposed and demonstrate its effectiveness with experimental data obtained from a VLSI Soft Winner-Take-All network.},
  keywords = {Biological neural networks,Couplings,Neuromorphics,Neurons,Silicon}
}

@article{neftci_systematic_2011,
  title = {A {{Systematic Method}} for {{Configuring VLSI Networks}} of {{Spiking Neurons}}},
  author = {Neftci, Emre and Chicca, Elisabetta and Indiveri, Giacomo and Douglas, Rodney},
  year = {2011},
  month = oct,
  journal = {Neural Comput.},
  volume = {23},
  number = {10},
  pages = {2457--2497},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00182},
  urldate = {2021-12-14},
  abstract = {An increasing number of research groups are developing custom hybrid analog/digital very large scale integration (VLSI) chips and systems that implement hundreds to thousands of spiking neurons with biophysically realistic dynamics, with the intention of emulating brainlike real-world behavior in hardware and robotic systems rather than simply simulating their performance on general-purpose digital computers. Although the electronic engineering aspects of these emulation systems is proceeding well, progress toward the actual emulation of brainlike tasks is restricted by the lack of suitable high-level configuration methods of the kind that have already been developed over many decades for simulations on general-purpose computers. The key difficulty is that the dynamics of the CMOS electronic analogs are determined by transistor biases that do not map simply to the parameter types and values used in typical abstract mathematical models of neurons and their networks. Here we provide a general method for resolving this difficulty. We describe a parameter mapping technique that permits an automatic configuration of VLSI neural networks so that their electronic emulation conforms to a higher-level neuronal simulation. We show that the neurons configured by our method exhibit spike timing statistics and temporal dynamics that are the same as those observed in the software simulated neurons and, in particular, that the key parameters of recurrent VLSI neural networks (e.g., implementing soft winner-take-all) can be precisely tuned. The proposed method permits a seamless integration between software simulations with hardware emulations and intertranslatability between the parameters of abstract neuronal models and their emulation counterparts. Most important, our method offers a route toward a high-level task configuration language for neuromorphic VLSI systems.}
}

@misc{noauthor_benjamincramerneuromorphic-bistability_nodate,
  title = {Benjamincramer/Neuromorphic-Bistability},
  journal = {GitHub},
  urldate = {2022-01-19},
  abstract = {Contribute to benjamincramer/neuromorphic-bistability development by creating an account on GitHub.},
  howpublished = {https://github.com/benjamincramer/neuromorphic-bistability},
  langid = {english}
}

@misc{noauthor_hmmlearnhmmlearn_2022,
  title = {Hmmlearn/Hmmlearn},
  year = {2022},
  month = dec,
  urldate = {2022-12-19},
  abstract = {Hidden Markov Models in Python, with scikit-learn like API},
  copyright = {BSD-3-Clause},
  howpublished = {hmmlearn}
}

@article{ostojic_two_2014,
  title = {Two Types of Asynchronous Activity in Networks of Excitatory and Inhibitory Spiking Neurons},
  author = {Ostojic, Srdjan},
  year = {2014},
  month = apr,
  journal = {Nat. Neurosci.},
  volume = {17},
  number = {4},
  pages = {594--600},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3658},
  urldate = {2021-12-14},
  abstract = {Here the author shows that an unstructured, sparsely connected network of model spiking neurons can display two different types of asynchronous activity: one in which an external input leads to a highly redundant response of different neurons that favors information transmission and another in which the firing rates of individual neurons fluctuate strongly in time and across neurons to provide a substrate for complex information processing.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Network models},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Network models Subject\_term\_id: network-models}
}

@article{parga_network_2007,
  title = {Network Model of Spontaneous Activity Exhibiting Synchronous Transitions between up and down States},
  author = {Parga, Nestor and Abbott, Larry},
  year = {2007},
  journal = {Front. Neurosci.},
  volume = {1},
  issn = {1662-453X},
  urldate = {2022-02-14}
}

@article{pastor-satorras_epidemic_2015,
  title = {Epidemic Processes in Complex Networks},
  author = {{Pastor-Satorras}, Romualdo and Castellano, Claudio and Van Mieghem, Piet and Vespignani, Alessandro},
  year = {2015},
  month = aug,
  journal = {Rev. Mod. Phys.},
  volume = {87},
  number = {3},
  pages = {925--979},
  issn = {0034-6861, 1539-0756},
  doi = {10.1103/RevModPhys.87.925},
  urldate = {2018-06-07},
  langid = {english}
}

@article{pehle_brainscales-2_2022,
  title = {The {{BrainScaleS-2 Accelerated Neuromorphic System With Hybrid Plasticity}}},
  author = {Pehle, Christian and Billaudelle, Sebastian and Cramer, Benjamin and Kaiser, Jakob and Schreiber, Korbinian and Stradmann, Yannik and Weis, Johannes and Leibfried, Aron and M{\"u}ller, Eric and Schemmel, Johannes},
  year = {2022},
  journal = {Front. Neurosci.},
  volume = {16},
  issn = {1662-453X},
  urldate = {2022-04-26},
  abstract = {Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks\textemdash sometimes referred to as the third generation of neural networks\textemdash are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network.}
}

@article{pehle_brainscales-2_2022-1,
  title = {The {{BrainScaleS-2 Accelerated Neuromorphic System With Hybrid Plasticity}}},
  author = {Pehle, Christian and Billaudelle, Sebastian and Cramer, Benjamin and Kaiser, Jakob and Schreiber, Korbinian and Stradmann, Yannik and Weis, Johannes and Leibfried, Aron and M{\"u}ller, Eric and Schemmel, Johannes},
  year = {2022},
  journal = {Front. Neurosci.},
  volume = {16},
  issn = {1662-453X},
  urldate = {2022-12-16},
  abstract = {Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks\textemdash sometimes referred to as the third generation of neural networks\textemdash are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network.}
}

@article{perez-nieves_neural_2020,
  title = {Neural Heterogeneity Promotes Robust Learning},
  author = {{Perez-Nieves}, Nicolas and Leung, Vincent C. H. and Dragotti, Pier Luigi and Goodman, Dan F. M.},
  year = {2020},
  month = dec,
  journal = {bioRxiv},
  pages = {2020.12.18.423468},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.18.423468},
  urldate = {2021-01-11},
  abstract = {{$<$}p{$>$}The brain has a hugely diverse, heterogeneous structure. By contrast, many functional neural models are homogeneous. We compared the performance of spiking neural networks trained to carry out difficult tasks, with varying degrees of heterogeneity. Introducing heterogeneity in membrane and synapse time constants substantially improved task performance, and made learning more stable and robust across multiple training methods, particularly for tasks with a rich temporal structure. In addition, the distribution of time constants in the trained networks closely matches those observed experimentally. We suggest that the heterogeneity observed in the brain may be more than just the byproduct of noisy processes, but rather may serve an active and important role in allowing animals to learn in changing environments.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@article{pinto_multiple_2020,
  title = {Multiple Timescales of Sensory-Evidence Accumulation across the Dorsal Cortex},
  author = {Pinto, Lucas and Tank, David W. and Brody, Carlos D.},
  year = {2020},
  month = dec,
  journal = {bioRxiv},
  pages = {2020.12.28.424600},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.28.424600},
  urldate = {2021-01-11},
  abstract = {{$<$}p{$>$}Decisions requiring the gradual accrual of sensory evidence appear to recruit widespread cortical areas. However, the nature of the contributions of different regions remains unclear. Here we trained mice to accumulate evidence over seconds while navigating in virtual reality, and optogenetically silenced the activity of many cortical areas during different brief trial epochs. We found that the inactivation of different areas primarily affected the evidence-accumulation computation per se, rather than other decision-related processes. Specifically, we observed selective changes in the weighting of evidence over time, such that frontal inactivations led to deficits on longer timescales than posterior cortical ones. Likewise, large-scale cortical Ca2+ activity during task performance displayed different temporal integration windows matching the effects of inactivation. Our findings suggest that distributed cortical areas accumulate evidence following their hierarchy of intrinsic timescales.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@article{pizzi_bistability_2021,
  title = {Bistability and Time Crystals in Long-Ranged Directed Percolation},
  author = {Pizzi, Andrea and Nunnenkamp, Andreas and Knolle, Johannes},
  year = {2021},
  month = feb,
  journal = {Nat. Commun.},
  volume = {12},
  number = {1},
  pages = {1061},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-21259-4},
  urldate = {2021-11-05},
  abstract = {Stochastic processes govern the time evolution of a huge variety of realistic systems throughout the sciences. A minimal description of noisy many-particle systems within a Markovian picture and with a notion of spatial dimension is given by probabilistic cellular automata, which typically feature time-independent and short-ranged update rules. Here, we propose a simple cellular automaton with power-law interactions that gives rise to a bistable phase of long-ranged directed percolation whose long-time behaviour is not only dictated by the system dynamics, but also by the initial conditions. In the presence of a periodic modulation of the update rules, we find that the system responds with a period larger than that of the modulation for an exponentially (in system size) long time. This breaking of discrete time translation symmetry of the underlying dynamics is enabled by a self-correcting mechanism of the long-ranged interactions which compensates noise-induced imperfections. Our work thus provides a firm example of a classical discrete time crystal phase of matter and paves the way for the study of novel non-equilibrium phases in the unexplored field of driven probabilistic cellular automata.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Condensed-matter physics,Statistical physics,thermodynamics and nonlinear dynamics},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Condensed-matter physics;Statistical physics, thermodynamics and nonlinear dynamics Subject\_term\_id: condensed-matter-physics;statistical-physics-thermodynamics-and-nonlinear-dynamics}
}

@book{pruessner_self-organised_2012,
  title = {Self-{{Organised Criticality}}: {{Theory}}, {{Models}} and {{Characterisation}}},
  shorttitle = {Self-{{Organised Criticality}}},
  author = {Pruessner, Gunnar},
  year = {2012},
  month = aug,
  publisher = {{Cambridge University Press}},
  abstract = {"When Bak, Tang, and Wiesenfeld (1987) coined the term Self-Organised Criticality (SOC), it was an explanation for an unexpected observation of scale invariance and at the same time, a programme of further research. Over the years it developed into a subject area which is concerned mostly with the analysis of computer models that display a form of generic scale invariance. The primacy of the computer model is manifest in the first publication and throughout the history of SOC, which evolved with and revolved around such computer models. That has led to a plethora of computer 'models', many of which are not intended to model much except themselves (also Gisiger, 2001), in the hope that they display a certain aspect of SOC in a particularly clear way. The question whether SOC exists is empty if SOC is merely the title for a certain class of computer models. In the following, the term SOC will therefore be used in its original meaning (Bak et al, 1987), to be assigned to systems with spatial degrees of freedom [which] naturally evolve into a self-organized critical point. Such behaviour is to be juxtaposed to the traditional notion of a phase transition, which is the singular, critical point in a phase diagram, where a system experiences a breakdown of symmetry and long-range spatial and in non-equilibrium, also temporal correlations, generally summarised as (power law) scaling (Widom, 1965a,b; Stanley, 1971)"--Publisher.},
  googlebooks = {TXKcpGqMSDYC},
  isbn = {978-0-521-85335-4},
  langid = {english},
  keywords = {Science / Mechanics / Fluids,Science / Physics / General,Science / Physics / Mathematical \& Computational,Science / System Theory}
}

@article{quian_quiroga_extracting_2009,
  title = {Extracting Information from Neuronal Populations: Information Theory and Decoding Approaches},
  shorttitle = {Extracting Information from Neuronal Populations},
  author = {Quian Quiroga, Rodrigo and Panzeri, Stefano},
  year = {2009},
  month = mar,
  journal = {Nat. Rev. Neurosci.},
  volume = {10},
  number = {3},
  pages = {173--185},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn2578},
  urldate = {2020-10-13},
  abstract = {To understand complex brain processes, there is a clear need to shift from traditional single-cell studies of trial-averaged responses to single-trial analyses of multiple neurons. In this respect, the decoding and information-theory formalisms offer a powerful framework to study how the brain computes information from the single-trial activity of neuronal populations.Compared with single-cell studies, population analysis with decoding and information theory has several advantages: the information of the neuronal population is considered as a whole; information is extracted from single-trial occurrences; it is possible to discover which stimulus features are encoded by the neural responses; it is possible to evaluate which features of the neural responses carry relevant information; and it is possible to combine information from different types of neural signals.Several studies have shown how much more knowledge can be extracted using the decoding and information-theory methodologies and how, in some cases, information that it is ambiguous at the single-cell level can be clearly interpreted when considering the whole population.Decoding has the advantage of being similar to real behavioural calculations, but it may lose information contained in the neural responses. Information theory considers all the information in the neural response, but it is more difficult to compute for large populations and its values may not be biologically relevant.The complementary knowledge offered by decoding and information theory has not been exploited enough in neuroscience. A joint application of both approaches may offer additional insights into how neuronal populations encode information.},
  copyright = {2009 Nature Publishing Group},
  langid = {english}
}

@article{rabiner_introduction_1986,
  title = {An Introduction to Hidden {{Markov}} Models},
  author = {Rabiner, L. and Juang, B.},
  year = {1986},
  month = jan,
  journal = {IEEE ASSP Mag.},
  volume = {3},
  number = {1},
  pages = {4--16},
  issn = {1558-1284},
  doi = {10.1109/MASSP.1986.1165342},
  abstract = {The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.},
  keywords = {Fluctuations,Hidden Markov models,Linear systems,Mathematical model,Optimization methods,Pattern matching,Speech processing,Speech recognition,Time varying systems}
}

@article{rajan_eigenvalue_2006,
  title = {Eigenvalue {{Spectra}} of {{Random Matrices}} for {{Neural Networks}}},
  author = {Rajan, Kanaka and Abbott, L. F.},
  year = {2006},
  month = nov,
  journal = {Phys. Rev. Lett.},
  volume = {97},
  number = {18},
  pages = {188104},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.188104},
  urldate = {2022-02-03},
  langid = {english}
}

@article{raut_hierarchical_2020,
  title = {Hierarchical Dynamics as a Macroscopic Organizing Principle of the Human Brain},
  author = {Raut, Ryan V. and Snyder, Abraham Z. and Raichle, Marcus E.},
  year = {2020},
  month = aug,
  journal = {Proc. Natl. Acad. Sci.},
  volume = {117},
  number = {34},
  pages = {20890--20897},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2003383117},
  urldate = {2021-12-14},
  abstract = {Multimodal evidence suggests that brain regions accumulate information over timescales that vary according to anatomical hierarchy. Thus, these experimentally defined ``temporal receptive windows'' are longest in cortical regions that are distant from sensory input. Interestingly, spontaneous activity in these regions also plays out over relatively slow timescales (i.e., exhibits slower temporal autocorrelation decay). These findings raise the possibility that hierarchical timescales represent an intrinsic organizing principle of brain function. Here, using resting-state functional MRI, we show that the timescale of ongoing dynamics follows hierarchical spatial gradients throughout human cerebral cortex. These intrinsic timescale gradients give rise to systematic frequency differences among large-scale cortical networks and predict individual-specific features of functional connectivity. Whole-brain coverage permitted us to further investigate the large-scale organization of subcortical dynamics. We show that cortical timescale gradients are topographically mirrored in striatum, thalamus, and cerebellum. Finally, timescales in the hippocampus followed a posterior-to-anterior gradient, corresponding to the longitudinal axis of increasing representational scale. Thus, hierarchical dynamics emerge as a global organizing principle of mammalian brains.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {32817467},
  keywords = {fMRI,frequency,functional connectivity,intrinsic,subcortex}
}

@article{reinhold_distinct_2015,
  title = {Distinct Recurrent versus Afferent Dynamics in Cortical Visual Processing},
  author = {Reinhold, Kimberly and Lien, Anthony D. and Scanziani, Massimo},
  year = {2015},
  month = dec,
  journal = {Nat. Neurosci.},
  volume = {18},
  number = {12},
  pages = {1789--1797},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4153},
  urldate = {2021-10-11},
  abstract = {How intracortical recurrent circuits in mammalian sensory cortex influence dynamics of sensory representation is not understood. Previous methods could not distinguish the relative contributions of recurrent circuits and thalamic afferents to cortical dynamics. We accomplish this by optogenetically manipulating thalamus and cortex. Over the initial 40 ms of visual stimulation, excitation from recurrent circuits in visual cortex progressively increased to exceed direct thalamocortical excitation. Even when recurrent excitation exceeded thalamic excitation, upon silencing thalamus, sensory-evoked activity in cortex decayed rapidly, with a time constant of 10 ms, which is similar to a neuron's integration time window. In awake mice, this cortical decay function predicted the time-locking of cortical activity to thalamic input at frequencies {$<$}15 Hz and attenuation of the cortical response to higher frequencies. Under anesthesia, depression at thalamocortical synapses disrupted the fidelity of sensory transmission. Thus, we determine dynamics intrinsic to cortical recurrent circuits that transform afferent input in time.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Striate cortex;Synaptic transmission;Thalamus Subject\_term\_id: striate-cortex;synaptic-transmission;thalamus}
}

@article{renart_asynchronous_2010,
  title = {The {{Asynchronous State}} in {{Cortical Circuits}}},
  author = {Renart, A. and {de la Rocha}, J. and Bartho, P. and Hollender, L. and Parga, N. and Reyes, A. and Harris, K. D.},
  year = {2010},
  month = jan,
  journal = {Science},
  volume = {327},
  number = {5965},
  pages = {587--590},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1179850},
  urldate = {2020-04-22},
  langid = {english}
}

@article{renart_mean-driven_2006,
  title = {Mean-{{Driven}} and {{Fluctuation-Driven Persistent Activity}} in {{Recurrent Networks}}},
  author = {Renart, Alfonso and {Moreno-Bote}, Rub{\'e}n and Wang, Xiao-Jing and Parga, N{\'e}stor},
  year = {2006},
  month = nov,
  journal = {Neural Comput.},
  volume = {19},
  number = {1},
  pages = {1--46},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.2007.19.1.1},
  urldate = {2020-03-24},
  abstract = {Spike trains from cortical neurons show a high degree of irregularity, with coefficients of variation (CV) of their interspike interval (ISI) distribution close to or higher than one. It has been suggested that this irregularity might be a reflection of a particular dynamical state of the local cortical circuit in which excitation and inhibition balance each other. In this ``balanced'' state, the mean current to the neurons is below threshold, and firing is driven by current fluctuations, resulting in irregular Poisson-like spike trains. Recent data show that the degree of irregularity in neuronal spike trains recorded during the delay period of working memory experiments is the same for both low-activity states of a few Hz and for elevated, persistent activity states of a few tens of Hz. Since the difference between these persistent activity states cannot be due to external factors coming from sensory inputs, this suggests that the underlying network dynamics might support coexisting balanced states at different firing rates. We use mean field techniques to study the possible existence of multiple balanced steady states in recurrent networks of current-based leaky integrate-and-fire (LIF) neurons. To assess the degree of balance of a steady state, we extend existing mean-field theories so that not only the firing rate, but also the coefficient of variation of the interspike interval distribution of the neurons, are determined self-consistently. Depending on the connectivity parameters of the network, we find bistable solutions of different types. If the local recurrent connectivity is mainly excitatory, the two stable steady states differ mainly in the mean current to the neurons. In this case, the mean drive in the elevated persistent activity state is suprathreshold and typically characterized by low spiking irregularity. If the local recurrent excitatory and inhibitory drives are both large and nearly balanced, or even dominated by inhibition, two stable states coexist, both with subthreshold current drive. In this case, the spiking variability in both the resting state and the mnemonic persistent state is large, but the balance condition implies parameter fine-tuning. Since the degree of required fine-tuning increases with network size and, on the other hand, the size of the fluctuations in the afferent current to the cells increases for small networks, overall we find that fluctuation-driven persistent activity in the very simplified type of models we analyze is not a robust phenomenon. Possible implications of considering more realistic models are discussed.}
}

@article{rosenbaum_spatial_2017,
  title = {The Spatial Structure of Correlated Neuronal Variability},
  author = {Rosenbaum, Robert and Smith, Matthew A. and Kohn, Adam and Rubin, Jonathan E. and Doiron, Brent},
  year = {2017},
  month = jan,
  journal = {Nat. Neurosci.},
  volume = {20},
  number = {1},
  pages = {107--114},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4433},
  urldate = {2022-01-19},
  abstract = {The activity of cortical neurons is extremely noisy. This study builds a mathematical theory linking the spatial scales of cortical wiring to how noise is generated and distributed over a population of neurons. Predictions from the theory are validated using population recordings in primate visual area V1.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Network models,Neural circuits},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Network models;Neural circuits Subject\_term\_id: network-models;neural-circuit}
}

@article{rudelt_embedding_2021,
  title = {Embedding Optimization Reveals Long-Lasting History Dependence in Neural Spiking Activity},
  author = {Rudelt, Lucas and Marx, Daniel Gonz{\'a}lez and Wibral, Michael and Priesemann, Viola},
  year = {2021},
  month = jan,
  journal = {PLOS Comput. Biol.},
  volume = {17},
  number = {6},
  pages = {e1008927},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008927},
  urldate = {2023-03-09},
  abstract = {Information processing can leave distinct footprints on the statistics of neural spiking. For example, efficient coding minimizes the statistical dependencies on the spiking history, while temporal integration of information may require the maintenance of information over different timescales. To investigate these footprints, we developed a novel approach to quantify history dependence within the spiking of a single neuron, using the mutual information between the entire past and current spiking. This measure captures how much past information is necessary to predict current spiking. In contrast, classical time-lagged measures of temporal dependence like the autocorrelation capture how long\textemdash potentially redundant\textemdash past information can still be read out. Strikingly, we find for model neurons that our method disentangles the strength and timescale of history dependence, whereas the two are mixed in classical approaches. When applying the method to experimental data, which are necessarily of limited size, a reliable estimation of mutual information is only possible for a coarse temporal binning of past spiking, a so-called past embedding. To still account for the vastly different spiking statistics and potentially long history dependence of living neurons, we developed an embedding-optimization approach that does not only vary the number and size, but also an exponential stretching of past bins. For extra-cellular spike recordings, we found that the strength and timescale of history dependence indeed can vary independently across experimental preparations. While hippocampus indicated strong and long history dependence, in visual cortex it was weak and short, while in vitro the history dependence was strong but short. This work enables an information-theoretic characterization of history dependence in recorded spike trains, which captures a footprint of information processing that is beyond time-lagged measures of temporal dependence. To facilitate the application of the method, we provide practical guidelines and a toolbox.},
  langid = {english},
  keywords = {Action potentials,Autocorrelation,Entropy,Neurons,Retina,Single neuron function,Time measurement,Visual cortex}
}

@article{scarpetta_hysteresis_2018,
  title = {Hysteresis, Neural Avalanches, and Critical Behavior near a First-Order Transition of a Spiking Neural Network},
  author = {Scarpetta, Silvia and Apicella, Ilenia and Minati, Ludovico and {de Candia}, Antonio},
  year = {2018},
  month = jun,
  journal = {Phys. Rev. E},
  volume = {97},
  number = {6},
  pages = {062305},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.97.062305},
  urldate = {2022-08-18},
  abstract = {Many experimental results, both in vivo and in vitro, support the idea that the brain cortex operates near a critical point and at the same time works as a reservoir of precise spatiotemporal patterns. However, the mechanism at the basis of these observations is still not clear. In this paper we introduce a model which combines both these features, showing that scale-free avalanches are the signature of a system posed near the spinodal line of a first-order transition, with many spatiotemporal patterns stored as dynamical metastable attractors. Specifically, we studied a network of leaky integrate-and-fire neurons whose connections are the result of the learning of multiple spatiotemporal dynamical patterns, each with a randomly chosen ordering of the neurons. We found that the network shows a first-order transition between a low-spiking-rate disordered state (down), and a high-rate state characterized by the emergence of collective activity and the replay of one of the stored patterns (up). The transition is characterized by hysteresis, or alternation of up and down states, depending on the lifetime of the metastable states. In both cases, critical features and neural avalanches are observed. Notably, critical phenomena occur at the edge of a discontinuous phase transition, as recently observed in a network of glow lamps.}
}

@book{scheffer_critical_2009,
  title = {Critical {{Transitions}} in {{Nature}} and {{Society}}},
  author = {Scheffer, Marten},
  year = {2009},
  journal = {Critical Transitions in Nature and Society},
  publisher = {{Princeton University Press}},
  doi = {10.1515/9781400833276},
  urldate = {2022-02-14},
  abstract = {How do we explain the remarkably abrupt changes that sometimes occur in nature and society--and can we predict why and when they happen? This book offers a comprehensive introduction to critical transitions in complex systems--the radical changes that happen at tipping points when thresholds are passed. Marten Scheffer accessibly describes the dynamical systems theory behind critical transitions, covering catastrophe theory, bifurcations, chaos, and more. He gives examples of critical transitions in lakes, oceans, terrestrial ecosystems, climate, evolution, and human societies. And he demonstrates how to deal with these transitions, offering practical guidance on how to predict tipping points, how to prevent "bad" transitions, and how to promote critical transitions that work for us and not against us. Scheffer shows the time is ripe for understanding and managing critical transitions in the vast and complex systems in which we live. This book can also serve as a textbook and includes a detailed appendix with equations. Provides an accessible introduction to dynamical systems theory Covers critical transitions in lakes, oceans, terrestrial ecosystems, the climate, evolution, and human societies Explains how to predict tipping points Offers strategies for preventing "bad" transitions and triggering "good" ones Features an appendix with equations},
  isbn = {978-1-4008-3327-6},
  langid = {english},
  keywords = {adaptive capacity,Allee effect,aquatic vegetation,biomanipulation,catastrophe fold,catastrophic shift,coral reefs,dynamical systems,eutrophication,extinction of species,fold bifurcation,functional group,glaciation cycles,Great Barrier Reef,Hopf bifurcation,ice sheet dynamics,invasion,kelp,Lake Alderfen Broad,microcredits,overexploitation,overgrazing,paradigm shifts,poverty trap,quasiperiodic oscillations,refuge,runaway process,Sahara desert,stability landscape,thermo-haline circulation,unstable equilibrium}
}

@article{schemmel_accelerated_2020,
  title = {Accelerated {{Analog Neuromorphic Computing}}},
  author = {Schemmel, Johannes and Billaudelle, Sebastian and Dauer, Phillip and Weis, Johannes},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.11996},
  eprint = {2003.11996},
  urldate = {2022-01-12},
  abstract = {This paper presents the concepts behind the BrainScales (BSS) accelerated analog neuromorphic computing architecture. It describes the second-generation BrainScales-2 (BSS-2) version and its most recent in-silico realization, the HICANN-X Application Specific Integrated Circuit (ASIC), as it has been developed as part of the neuromorphic computing activities within the European Human Brain Project (HBP). While the first generation is implemented in an 180nm process, the second generation uses 65nm technology. This allows the integration of a digital plasticity processing unit, a highly-parallel micro processor specially built for the computational needs of learning in an accelerated analog neuromorphic systems. The presented architecture is based upon a continuous-time, analog, physical model implementation of neurons and synapses, resembling an analog neuromorphic accelerator attached to build-in digital compute cores. While the analog part emulates the spike-based dynamics of the neural network in continuous-time, the latter simulates biological processes happening on a slower time-scale, like structural and parameter changes. Compared to biological time-scales, the emulation is highly accelerated, i.e. all time-constants are several orders of magnitude smaller than in biology. Programmable ion channel emulation and inter-compartmental conductances allow the modeling of nonlinear dendrites, back-propagating action-potentials as well as NMDA and Calcium plateau potentials. To extend the usability of the analog accelerator, it also supports vector-matrix multiplication. Thereby, BSS-2 supports inference of deep convolutional networks as well as local-learning with complex ensembles of spiking neurons within the same substrate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition}
}

@incollection{schemmel_accelerated_2022,
  title = {Accelerated {{Analog Neuromorphic Computing}}},
  booktitle = {Analog {{Circuits}} for {{Machine Learning}}, {{Current}}/{{Voltage}}/{{Temperature Sensors}}, and {{High-speed Communication}}: {{Advances}} in {{Analog Circuit Design}} 2021},
  author = {Schemmel, Johannes and Billaudelle, Sebastian and Dauer, Philipp and Weis, Johannes},
  editor = {Harpe, Pieter and Makinwa, Kofi A.A. and Baschirotto, Andrea},
  year = {2022},
  pages = {83--102},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-91741-8_6},
  urldate = {2022-05-17},
  abstract = {This chapter presents the concepts behind the BrainScales (BSS) accelerated analog neuromorphic computing architecture. It describes the second-generation BrainScales-2 (BSS-2) version and its most recent in silico realization, the HICANN-X Application Specific Integrated Circuit (ASIC), as it has been developed as part of the neuromorphic computing activities within the European Human Brain Project (HBP). While the first generation is implemented in a 180 nm process, the second generation uses 65 nm technology. This allows the integration of a digital plasticity processing unit, a highly parallel microprocessor specially built for the computational needs of learning in an accelerated analog neuromorphic systems.},
  isbn = {978-3-030-91741-8},
  langid = {english},
  keywords = {Analog,Analog inference,Analog to Digital Converters (ADC),BrainScaleS (BSS),Digital to Analog Converter (DAC),Event-routing,Integrate-and-Fire,Neuromorphic architecture,Neuromorphic computing,Plasticity Processing Units (PPU),PyNN,Python,Single Instruction Multiple Data (SIMD),Spike-timing dependent plasticity (STDP),Tsodys-Markram model,Vector-matrix multiplication}
}

@inproceedings{schemmel_modeling_2007,
  title = {Modeling {{Synaptic Plasticity}} within {{Networks}} of {{Highly Accelerated I}}\&{{F Neurons}}},
  booktitle = {2007 {{IEEE Int}}. {{Symp}}. {{Circuits Syst}}.},
  author = {Schemmel, Johannes and Bruderle, Daniel and Meier, Karlheinz and Ostendorf, Boris},
  year = {2007},
  month = may,
  pages = {3367--3370},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2007.378289},
  abstract = {When studying the different aspects of synaptic plasticity, the timescales involved range from milliseconds to hours, thus covering at least seven orders of magnitude. To make this temporal dynamic range accessible to the experimentalist, we have developed a highly accelerated analog VLSI model of leaky integrate and fire neurons. It incorporates fast and slow synaptic facilitation and depression mechanisms in its conductance based synapses. By using a 180 nm process 105 synapses fit on a 25 mm2 die. A single chip can model the temporal evolution of the synaptic weights in networks of up to 384 neurons with an acceleration factor of 105 while recording the neural action potentials with a temporal resolution better than 30 {$\mu$}s biological time. This reduces the time needed for a 10 minute experiment to merely 6 ms, paving the way for complex parameter searches to reproduce biological findings. Due to a digital communication structure larger networks can be built from multiple chips while retaining an acceleration factor of a least 104.},
  keywords = {Acceleration,Biological system modeling,Biomembranes,Circuits,Evolution (biology),Fires,Neurons,Neurotransmitters,Physics,Very large scale integration}
}

@inproceedings{schemmel_wafer-scale_2010,
  title = {A Wafer-Scale Neuromorphic Hardware System for Large-Scale Neural Modeling},
  booktitle = {2010 {{IEEE Int}}. {{Symp}}. {{Circuits Syst}}. {{ISCAS}}},
  author = {Schemmel, Johannes and Br{\"u}derle, Daniel and Gr{\"u}bl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
  year = {2010},
  month = may,
  pages = {1947--1950},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2010.5536970},
  abstract = {Modeling neural tissue is an important tool to investigate biological neural networks. Until recently, most of this modeling has been done using numerical methods. In the European research project "FACETS" this computational approach is complemented by different kinds of neuromorphic systems. A special emphasis lies in the usability of these systems for neuroscience. To accomplish this goal an integrated software/hardware framework has been developed which is centered around a unified neural system description language, called PyNN, that allows the scientist to describe a model and execute it in a transparent fashion on either a neuromorphic hardware system or a numerical simulator. A very large analog neuromorphic hardware system developed within FACETS is able to use complex neural models as well as realistic network topologies, i.e. it can realize more than 10000 synapses per neuron, to allow the direct execution of models which previously could have been simulated numerically only.},
  keywords = {Biological neural networks,Biological system modeling,Biological tissues,Biology computing,Hardware,Large-scale systems,Neuromorphics,Numerical simulation,Semiconductor device modeling,Usability}
}

@misc{schuman_survey_2017-1,
  title = {A {{Survey}} of {{Neuromorphic Computing}} and {{Neural Networks}} in {{Hardware}}},
  author = {Schuman, Catherine D. and Potok, Thomas E. and Patton, Robert M. and Birdwell, J. Douglas and Dean, Mark E. and Rose, Garrett S. and Plank, James S.},
  year = {2017},
  month = may,
  number = {arXiv:1705.06963},
  eprint = {1705.06963},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1705.06963},
  urldate = {2022-05-17},
  abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Neural and Evolutionary Computing}
}

@article{sharma_stochasticity_2015,
  title = {Stochasticity and Bistability in Insect Outbreak Dynamics},
  author = {Sharma, Yogita and Abbott, Karen C. and Dutta, Partha Sharathi and Gupta, A. K.},
  year = {2015},
  month = may,
  journal = {Theor. Ecol.},
  volume = {8},
  number = {2},
  pages = {163--174},
  issn = {1874-1746},
  doi = {10.1007/s12080-014-0241-9},
  urldate = {2021-09-17},
  abstract = {There is a long history in ecology of using mathematical models to identify deterministic processes that may lead to dramatic population dynamic patterns like boom-and-bust outbreaks. Stochasticity is also well-known to have a significant influence on the dynamics of many ecological systems, but this aspect has received far less attention. Here, we study a stochastic version of a classic bistable insect outbreak model to reveal the role of stochasticity in generating outbreak dynamics. We find that stochasticity has strong effects on the dynamics and that the stochastic system can behave in ways that are not easily anticipated by its deterministic counterpart. Both the intensity and autocorrelation of the stochastic environment are important. Stochasticity with higher intensity (variability) generally weakens bistability, causing the dynamics to spend more time at a single state rather than jumping between alternative stable states. Which state the population tends toward depends on the noise color. High-intensity white noise causes the insect population to spend more time at low density, potentially reducing the severity or frequency of outbreaks. However, red (positively autocorrelated) noise can make the population spend more time near the high density state, intensifying outbreaks. Under neither type of noise do early warning signals reliably predict impending outbreaks or population crashes.},
  langid = {english}
}

@article{siegle_survey_2021,
  title = {Survey of Spiking in the Mouse Visual System Reveals Functional Hierarchy},
  author = {Siegle, Joshua H. and Jia, Xiaoxuan and Durand, S{\'e}verine and Gale, Sam and Bennett, Corbett and Graddis, Nile and Heller, Greggory and Ramirez, Tamina K. and Choi, Hannah and Luviano, Jennifer A. and Groblewski, Peter A. and Ahmed, Ruweida and Arkhipov, Anton and Bernard, Amy and Billeh, Yazan N. and Brown, Dillan and Buice, Michael A. and Cain, Nicolas and Caldejon, Shiella and Casal, Linzy and Cho, Andrew and Chvilicek, Maggie and Cox, Timothy C. and Dai, Kael and Denman, Daniel J. and {de Vries}, Saskia E. J. and Dietzman, Roald and Esposito, Luke and Farrell, Colin and Feng, David and Galbraith, John and Garrett, Marina and Gelfand, Emily C. and Hancock, Nicole and Harris, Julie A. and Howard, Robert and Hu, Brian and Hytnen, Ross and Iyer, Ramakrishnan and Jessett, Erika and Johnson, Katelyn and Kato, India and Kiggins, Justin and Lambert, Sophie and Lecoq, Jerome and Ledochowitsch, Peter and Lee, Jung Hoon and Leon, Arielle and Li, Yang and Liang, Elizabeth and Long, Fuhui and Mace, Kyla and Melchior, Jose and Millman, Daniel and Mollenkopf, Tyler and Nayan, Chelsea and Ng, Lydia and Ngo, Kiet and Nguyen, Thuyahn and Nicovich, Philip R. and North, Kat and Ocker, Gabriel Koch and Ollerenshaw, Doug and Oliver, Michael and Pachitariu, Marius and Perkins, Jed and Reding, Melissa and Reid, David and Robertson, Miranda and Ronellenfitch, Kara and Seid, Sam and Slaughterbeck, Cliff and Stoecklin, Michelle and Sullivan, David and Sutton, Ben and Swapp, Jackie and Thompson, Carol and Turner, Kristen and Wakeman, Wayne and Whitesell, Jennifer D. and Williams, Derric and Williford, Ali and Young, Rob and Zeng, Hongkui and Naylor, Sarah and Phillips, John W. and Reid, R. Clay and Mihalas, Stefan and Olsen, Shawn R. and Koch, Christof},
  year = {2021},
  month = apr,
  journal = {Nature},
  volume = {592},
  number = {7852},
  pages = {86--92},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03171-x},
  urldate = {2021-12-14},
  abstract = {The anatomy of the mammalian visual system, from the retina to the neocortex, is organized hierarchically1. However, direct observation of cellular-level functional interactions across this hierarchy is lacking due to the challenge of simultaneously recording activity across numerous regions. Here we describe a large, open dataset\textemdash part of the Allen Brain Observatory2\textemdash that surveys spiking from tens of thousands of units in six cortical and two thalamic regions in the brains of mice responding to a battery of visual stimuli. Using cross-correlation analysis, we reveal that the organization of inter-area functional connectivity during visual stimulation mirrors the anatomical hierarchy from the Allen Mouse Brain Connectivity Atlas3. We find that four classical hierarchical measures\textemdash response latency, receptive-field size, phase-locking to drifting gratings and response decay timescale\textemdash are all correlated with the hierarchy. Moreover, recordings obtained during a visual task reveal that the correlation between neural activity and behavioural choice also increases along the hierarchy. Our study provides a foundation for understanding coding and signal propagation across hierarchically organized cortical and thalamic visual areas.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Neural circuits,Sensory processing,Visual system},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Neural circuits;Sensory processing;Visual system Subject\_term\_id: neural-circuit;sensory-processing;visual-system}
}

@article{softky_highly_1993,
  title = {The Highly Irregular Firing of Cortical Cells Is Inconsistent with Temporal Integration of Random {{EPSPs}}},
  author = {Softky, W. R. and Koch, C.},
  year = {1993},
  month = jan,
  journal = {J. Neurosci.},
  volume = {13},
  number = {1},
  pages = {334--350},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.13-01-00334.1993},
  urldate = {2022-02-14},
  abstract = {How random is the discharge pattern of cortical neurons? We examined recordings from primary visual cortex (V1; Knierim and Van Essen, 1992) and extrastriate cortex (MT; Newsome et al., 1989a) of awake, behaving macaque monkey and compared them to analytical predictions. For nonbursting cells firing at sustained rates up to 300 Hz, we evaluated two indices of firing variability: the ratio of the variance to the mean for the number of action potentials evoked by a constant stimulus, and the rate-normalized coefficient of variation (Cv) of the interspike interval distribution. Firing in virtually all V1 and MT neurons was nearly consistent with a completely random process (e.g., Cv approximately 1). We tried to model this high variability by small, independent, and random EPSPs converging onto a leaky integrate-and- fire neuron (Knight, 1972). Both this and related models predicted very low firing variability (Cv {$<<$} 1) for realistic EPSP depolarizations and membrane time constants. We also simulated a biophysically very detailed compartmental model of an anatomically reconstructed and physiologically characterized layer V cat pyramidal cell (Douglas et al., 1991) with passive dendrites and active soma. If independent, excitatory synaptic input fired the model cell at the high rates observed in monkey, the Cv and the variability in the number of spikes were both very low, in agreement with the integrate-and-fire models but in strong disagreement with the majority of our monkey data. The simulated cell only produced highly variable firing when Hodgkin-Huxley- like currents (INa and very strong IDR) were placed on distal dendrites. Now the simulated neuron acted more as a millisecond- resolution detector of dendritic spike coincidences than as a temporal integrator. We argue that neurons that act as temporal integrators over many synaptic inputs must fire very regularly. Only in the presence of either fast and strong dendritic nonlinearities or strong synchronization among individual synaptic events will the degree of predicted variability approach that of real cortical neurons.},
  chapter = {Articles},
  copyright = {\textcopyright{} 1993 by Society for Neuroscience},
  langid = {english},
  pmid = {8423479}
}

@article{spitmaan_multiple_2020,
  title = {Multiple Timescales of Neural Dynamics and Integration of Task-Relevant Signals across Cortex},
  author = {Spitmaan, Mehran and Seo, Hyojung and Lee, Daeyeol and Soltani, Alireza},
  year = {2020},
  month = sep,
  journal = {Proc. Natl. Acad. Sci.},
  volume = {117},
  number = {36},
  pages = {22522--22531},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2005993117},
  urldate = {2021-12-14},
  abstract = {A long-lasting challenge in neuroscience has been to find a set of principles that could be used to organize the brain into distinct areas with specific functions. Recent studies have proposed the orderly progression in the time constants of neural dynamics as an organizational principle of cortical computations. However, relationships between these timescales and their dependence on response properties of individual neurons are unknown, making it impossible to determine how mechanisms underlying such a computational principle are related to other aspects of neural processing. Here, we developed a comprehensive method to simultaneously estimate multiple timescales in neuronal dynamics and integration of task-relevant signals along with selectivity to those signals. By applying our method to neural and behavioral data during a dynamic decision-making task, we found that most neurons exhibited multiple timescales in their response, which consistently increased from parietal to prefrontal and cingulate cortex. While predicting rates of behavioral adjustments, these timescales were not correlated across individual neurons in any cortical area, resulting in independent parallel hierarchies of timescales. Additionally, none of these timescales depended on selectivity to task-relevant signals. Our results not only suggest the existence of multiple canonical mechanisms for increasing timescales of neural dynamics across cortex but also point to additional mechanisms that allow decorrelation of these timescales to enable more flexibility.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {32839338},
  keywords = {neural heterogeneity,prefrontal cortex,reward integration}
}

@article{stein_neuronal_2005,
  title = {Neuronal Variability: Noise or Part of the Signal?},
  shorttitle = {Neuronal Variability},
  author = {Stein, Richard B. and Gossen, E. Roderich and Jones, Kelvin E.},
  year = {2005},
  month = may,
  journal = {Nat. Rev. Neurosci.},
  volume = {6},
  number = {5},
  pages = {389--397},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn1668},
  urldate = {2022-02-14},
  abstract = {Traditionally, the rate of nerve impulses (spikes) over time was considered to be the main carrier of information in the nervous system. Therefore, any variability in the rate of response to a steady stimulus would reduce the information conveyed by a nerve cell. Many nerve cells fire with considerable variability, which would limit their ability to carry information to 2 or 3 bits in 1 s.With time-varying inputs containing the range of frequencies that the neuron responds to, values of information transmission of approximately 1 bit per spike have been calculated. For a neuron that fires tens or hundreds of spikes per second, much higher bit rates are possible than with steady inputs.Variability might also offer distinct advantages in preventing the entrainment of neurons to high-frequency signals. Enhanced sensitivity to weak signals has been proposed, which is known as 'stochastic resonance', as well as a role of variability in the method of Bayesian inference. Recent work on various sensory systems has emphasized the importance of timing, particularly that of first spikes, rather than the rate of firing over time.Rate coding might be more important in the motor system than precise timing. The variability in rate fluctuates with the mean rate (signal-dependent noise). The variability in the motor output in the presence of this noise can be minimized using optimal control theory.Optimal control theory predicts the form of many movements if a specific rule is assumed that relates the standard deviation in rate to the mean rate. This rule is not observed experimentally for either motor neurons or the motor cortex. However, the relationship between the standard deviation in muscle force and the mean force obeys the rule.The reason for the difference between the neural responses and the force output arises from the Henneman size principle. This states that the first recruited motor units are small and, hence, produce minor variations in force. Later motor units are larger and produce greater variations with the magnitude required by the optimal control theory.In the central nervous system, large excitatory postsynaptic potentials (EPSPs) can cause the near synchronous firing of groups of cells that might be important in attention, as well as learning and memory. Interactions in some areas, such as the hippocampus, between ongoing oscillations and spike activity might be used by 'place neurons' to locate the position of the body in external space. Therefore, variability in the firing rate of individual neurons is not simply noise, but might have a range of functions in neurons throughout the nervous system.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences}
}

@article{stern_spontaneous_1997,
  title = {Spontaneous {{Subthreshold Membrane Potential Fluctuations}} and {{Action Potential Variability}} of {{Rat Corticostriatal}} and {{Striatal Neurons In Vivo}}},
  author = {Stern, Edward A. and Kincaid, Anthony E. and Wilson, Charles J.},
  year = {1997},
  month = apr,
  journal = {J. Neurophysiol.},
  volume = {77},
  number = {4},
  pages = {1697--1715},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.1997.77.4.1697},
  urldate = {2022-01-19},
  abstract = {Stern, Edward A., Anthony E. Kincaid, and Charles J. Wilson. Spontaneous subthreshold membrane potential fluctuations and action potential variability of rat corticostriatal and striatal neurons in vivo. J. Neurophysiol. 77: 1697\textendash 1715, 1997. We measured the timing of spontaneous membrane potential fluctuations and action potentials of medial and lateral agranular corticostriatal and striatal neurons with the use of in vivo intracellular recordings in urethan-anesthetized rats. All neurons showed spontaneous subthreshold membrane potential shifts from 7 to 32 mV in amplitude, fluctuating between a hyperpolarized down state and depolarized up state. Action potentials arose only during the up state. The membrane potential state transitions showed a weak periodicity with a peak frequency near 1 Hz. The peak of the frequency spectra was broad in all neurons, indicating that the membrane potential fluctuations were not dominated by a single periodic function. At frequencies {$>$}1 Hz, the log of magnitude decreased linearly with the log of frequency in all neurons. No serial dependence was found for up and down state durations, or for the time between successive up or down state transitions, showing that the up and down state transitions are not due to superimposition of noisy inputs onto a single frequency. Monte Carlo simulations of stochastic synaptic inputs to a uniform finite cylinder showed that the Fourier spectra obtained for corticostriatal and striatal neurons are inconsistent with a Poisson-like synaptic input, demonstrating that the up state is not due to an increase in the strength of an unpatterned synaptic input. Frequency components arising from state transitions were separated from those arising from the smaller membrane potential fluctuations within each state. A larger proportion of the total signal was represented by the fluctuations within states, especially in the up state, than was predicted by the simulations. The individual state spectra did not correspond to those of random synaptic inputs, but reproduced the spectra of the up and down state transitions. This suggests that the process causing the state transitions and the process responsible for synaptic input may be the same. A high-frequency periodic component in the up states was found in the majority of the corticostriatal cells in the sample. The average size of the component was not different between neurons injected with QX-314 and control neurons. The high-frequency component was not seen in any of our sample of striatal cells. Corticostriatal and striatal neurons' coefficients of variation of interspike intervals ranged from 1.0 to 1.9. When interspike intervals including a down state were subtracted from the calculation, the coefficient of variation ranged from 0.4 to 1.1, indicating that a substantial proportion of spike interval variance was due to the subthreshold membrane potential fluctuations.}
}

@article{stevens_input_1998,
  title = {Input Synchrony and the Irregular Firing of Cortical Neurons},
  author = {Stevens, Charles F. and Zador, Anthony M.},
  year = {1998},
  month = jul,
  journal = {Nat. Neurosci.},
  volume = {1},
  number = {3},
  pages = {210--217},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/659},
  urldate = {2022-02-14},
  abstract = {Cortical neurons in the waking brain fire highly irregular, seemingly random, spike trains in response to constant sensory stimulation, whereas  in vitro they fire regularly in response to constant current injection. To test whether, as has been suggested, this high in vivo variability could be due to the postsynaptic currents generated by independent synaptic inputs, we injected synthetic synaptic current into neocortical neurons in brain slices. We report that independent inputs cannot account for this high variability, but this variability can be explained by a simple alternative model of the synaptic drive in which inputs arrive synchronously. Our results suggest that synchrony may be important in the neural code by providing a means for encoding signals with high temporal fidelity over a population of neurons.},
  copyright = {1998 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences}
}

@article{stimberg_brian_2013,
  title = {Brian 2 - the Second Coming: Spiking Neural Network Simulation in {{Python}} with Code Generation},
  shorttitle = {Brian 2 - the Second Coming},
  author = {Stimberg, Marcel and Goodman, Dan FM and Benichoux, Victor and Brette, Romain},
  year = {2013},
  month = jul,
  journal = {BMC Neurosci.},
  volume = {14},
  number = {1},
  pages = {P38},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-14-S1-P38},
  urldate = {2021-03-03},
  langid = {english}
}

@article{stimberg_brian_2019,
  title = {Brian 2, an Intuitive and Efficient Neural Simulator},
  author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
  editor = {Skinner, Frances K and Calabrese, Ronald L and Skinner, Frances K and Zeldenrust, Fleur and Gerkin, Richard C},
  year = {2019},
  month = aug,
  journal = {eLife},
  volume = {8},
  pages = {e47314},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.47314},
  urldate = {2021-11-24},
  abstract = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
  keywords = {computational neuroscience,simulation,software}
}

@article{suchorski_role_2018,
  title = {The Role of Metal/Oxide Interfaces for Long-Range Metal Particle Activation during {{CO}} Oxidation},
  author = {Suchorski, Yuri and Kozlov, Sergey M. and Bespalov, Ivan and Datler, Martin and Vogel, Diana and Budinska, Zuzana and Neyman, Konstantin M. and Rupprechter, G{\"u}nther},
  year = {2018},
  month = jun,
  journal = {Nat. Mater.},
  volume = {17},
  number = {6},
  pages = {519--522},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4660},
  doi = {10.1038/s41563-018-0080-y},
  urldate = {2022-02-09},
  abstract = {Electron microscopy and modelling are used to study CO oxidation on oxide-supported Pd. The perimeter of the metal/oxide interface is shown to affect CO tolerance of the entire particle, demonstrating a long-range effect over micrometre length scales.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Materials for energy and catalysis,Nanoscience and technology}
}

@article{sukenik_neuronal_2021,
  title = {Neuronal Circuits Overcome Imbalance in Excitation and Inhibition by Adjusting Connection Numbers},
  author = {Sukenik, Nirit and Vinogradov, Oleg and Weinreb, Eyal and Segal, Menahem and Levina, Anna and Moses, Elisha},
  year = {2021},
  month = mar,
  journal = {Proc. Natl. Acad. Sci.},
  volume = {118},
  number = {12},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2018459118},
  urldate = {2021-03-17},
  abstract = {The interplay between excitation and inhibition is crucial for neuronal circuitry in the brain. Inhibitory cell fractions in the neocortex and hippocampus are typically maintained at 15 to 30\%, which is assumed to be important for stable dynamics. We have studied systematically the role of precisely controlled excitatory/inhibitory (E/I) cellular ratios on network activity using mice hippocampal cultures. Surprisingly, networks with varying E/I ratios maintain stable bursting dynamics. Interburst intervals remain constant for most ratios, except in the extremes of 0 to 10\% and 90 to 100\% inhibitory cells. Single-cell recordings and modeling suggest that networks adapt to chronic alterations of E/I compositions by balancing E/I connectivity. Gradual blockade of inhibition substantiates the agreement between the model and experiment and defines its limits. Combining measurements of population and single-cell activity with theoretical modeling, we provide a clearer picture of how E/I balance is preserved and where it fails in living neuronal networks.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2021 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {33723048},
  keywords = {bursting,E/I balance,network dynamics,neuronal network}
}

@article{takeuchi_directed_2007,
  title = {Directed {{Percolation Criticality}} in {{Turbulent Liquid Crystals}}},
  author = {Takeuchi, Kazumasa A. and Kuroda, Masafumi and Chat{\'e}, Hugues and Sano, Masaki},
  year = {2007},
  month = dec,
  journal = {Phys. Rev. Lett.},
  volume = {99},
  number = {23},
  pages = {234503},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.99.234503},
  urldate = {2022-02-09},
  abstract = {We experimentally investigate the critical behavior of a phase transition between two topologically different turbulent states of electrohydrodynamic convection in nematic liquid crystals. The statistical properties of the observed spatiotemporal intermittency regimes are carefully determined, yielding a complete set of static critical exponents in full agreement with those defining the directed percolation class in 2+1 dimensions. This constitutes the first clear and comprehensive experimental evidence of an absorbing phase transition in this prominent nonequilibrium universality class.}
}

@article{takeuchi_experimental_2009,
  title = {Experimental Realization of Directed Percolation Criticality in Turbulent Liquid Crystals},
  author = {Takeuchi, Kazumasa A. and Kuroda, Masafumi and Chat{\'e}, Hugues and Sano, Masaki},
  year = {2009},
  month = nov,
  journal = {Phys. Rev. E},
  volume = {80},
  number = {5},
  pages = {051116},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.80.051116},
  urldate = {2022-02-09},
  abstract = {This is a comprehensive report on the phase transition between two turbulent states of electroconvection in nematic liquid crystals, which was recently found by the authors to be in the directed percolation (DP) universality class [K. A. Takeuchi et al., Phys. Rev. Lett. 99, 234503 (2007)]. We further investigate both static and dynamic critical behaviors of this phase transition, measuring a total of 12 critical exponents, 5 scaling functions, and 8 scaling relations, all in full agreement with those characterizing the DP class in 2+1 dimensions. Developing an experimental technique to create a seed of topological-defect turbulence by pulse laser, we confirm in particular the rapidity symmetry, which is a basic but nontrivial consequence of the field-theoretic approach to DP. This provides a clear experimental realization of this outstanding truly out-of-equilibrium universality class, dominating most phase transitions into an absorbing state.}
}

@article{tartaglia_bistability_2017,
  title = {Bistability and up/down State Alternations in Inhibition-Dominated Randomly Connected Networks of {{LIF}} Neurons},
  author = {Tartaglia, Elisa M. and Brunel, Nicolas},
  year = {2017},
  month = sep,
  journal = {Sci. Rep.},
  volume = {7},
  number = {1},
  pages = {11916},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-12033-y},
  urldate = {2021-09-13},
  abstract = {Electrophysiological recordings in cortex in vivo have revealed a rich variety of dynamical regimes ranging from irregular asynchronous states to a diversity of synchronized states, depending on species, anesthesia, and external stimulation. The average population firing rate in these states is typically low. We study analytically and numerically a network of sparsely connected excitatory and inhibitory integrate-and-fire neurons in the inhibition-dominated, low firing rate regime. For sufficiently high values of the external input, the network exhibits an asynchronous low firing frequency state (L). Depending on synaptic time constants, we show that two scenarios may occur when external inputs are decreased: (1) the L state can destabilize through a Hopf bifucation as the external input is decreased, leading to synchronized oscillations spanning d {$\delta$} to {$\beta$} frequencies; (2) the network can reach a bistable region, between the low firing frequency network state (L) and a quiescent one (Q). Adding an adaptation current to excitatory neurons leads to spontaneous alternations between L and Q states, similar to experimental observations on UP and DOWN states alternations.},
  copyright = {2017 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Dynamical systems;Network models Subject\_term\_id: dynamical-systems;network-models}
}

@article{turrigiano_activity-dependent_1998,
  title = {Activity-Dependent Scaling of Quantal Amplitude in Neocortical Neurons},
  author = {Turrigiano, Gina G. and Leslie, Kenneth R. and Desai, Niraj S. and Rutherford, Lana C. and Nelson, Sacha B.},
  year = {1998},
  month = feb,
  journal = {Nature},
  volume = {391},
  number = {6670},
  pages = {892--896},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/36103},
  urldate = {2021-12-14},
  abstract = {Information is stored in neural circuits through long-lasting changes in synaptic strengths1,2. Most studies of information storage have focused on mechanisms such as long-term potentiation and depression (LTP and LTD), in which synaptic strengths change in a synapse-specific manner3,4. In contrast, little attention has been paid to mechanisms that regulate the total synaptic strength of a neuron. Here we describe a new form of synaptic plasticity that increases or decreases the strength of all of a neuron's synaptic inputs as a function of activity. Chronic blockade of cortical culture activity increased the amplitude of miniature excitatory postsynaptic currents (mEPSCs) without changing their kinetics. Conversely, blocking GABA ({$\gamma$}-aminutyric acid)-mediated inhibition initially raised firing rates, but over a 48-hour period mESPC amplitudes decreased and firing rates returned to close to control values. These changes were at least partly due to postsynaptic alterations in the response to glutamate, and apparently affected each synapse in proportion to its initial strength. Such `synaptic scaling' may help to ensure that firing rates do not become saturated during developmental changes in the number and strength of synaptic inputs5, as well as stabilizing synaptic strengths during Hebbian modification6,7 and facilitating competition between synapses7,8,9.},
  copyright = {1998 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research}
}

@article{turrigiano_homeostatic_2004,
  title = {Homeostatic Plasticity in the Developing Nervous System},
  author = {Turrigiano, Gina G. and Nelson, Sacha B.},
  year = {2004},
  month = feb,
  journal = {Nat. Rev. Neurosci.},
  volume = {5},
  number = {2},
  pages = {97--107},
  issn = {1471-0048},
  doi = {10.1038/nrn1327},
  urldate = {2019-04-19},
  abstract = {Activity has an important role in refining synaptic connectivity during development, in part through 'Hebbian' mechanisms such as long-term potentiation and long-term depression. However, Hebbian plasticity is probably insufficient to explain activity-dependent development because it tends to destabilize the activity of neural circuits. How can complex circuits maintain stable activity states in the face of such destabilizing forces? An idea that is emerging from recent work is that average neuronal activity levels are maintained by a set of homeostatic plasticity mechanisms that dynamically adjust synaptic strengths in the correct direction to promote stability. Here we discuss evidence from a number of systems that homeostatic synaptic plasticity is crucial for processes ranging from memory storage to activity-dependent development.},
  copyright = {2004 Nature Publishing Group},
  langid = {english}
}

@article{turrigiano_homeostatic_2012,
  title = {Homeostatic {{Synaptic Plasticity}}: {{Local}} and {{Global Mechanisms}} for {{Stabilizing Neuronal Function}}},
  shorttitle = {Homeostatic {{Synaptic Plasticity}}},
  author = {Turrigiano, Gina},
  year = {2012},
  month = jan,
  journal = {Cold Spring Harb. Perspect. Biol.},
  volume = {4},
  number = {1},
  pages = {a005736},
  publisher = {{Cold Spring Harbor Lab}},
  issn = {, 1943-0264},
  doi = {10.1101/cshperspect.a005736},
  urldate = {2022-01-14},
  abstract = {Neural circuits must maintain stable function in the face of many plastic challenges, including changes in synapse number and strength, during learning and development. Recent work has shown that these destabilizing influences are counterbalanced by homeostatic plasticity mechanisms that act to stabilize neuronal and circuit activity. One such mechanism is synaptic scaling, which allows neurons to detect changes in their own firing rates through a set of calcium-dependent sensors that then regulate receptor trafficking to increase or decrease the accumulation of glutamate receptors at synaptic sites. Additional homeostatic mechanisms may allow local changes in synaptic activation to generate local synaptic adaptations, and network-wide changes in activity to generate network-wide adjustments in the balance between excitation and inhibition. The signaling pathways underlying these various forms of homeostatic plasticity are currently under intense scrutiny, and although dozens of molecular pathways have now been implicated in homeostatic plasticity, a clear picture of how homeostatic feedback is structured at the molecular level has not yet emerged. On a functional level, neuronal networks likely use this complex set of regulatory mechanisms to achieve homeostasis over a wide range of temporal and spatial scales.},
  langid = {english},
  pmid = {22086977}
}

@article{van_meegen_large-deviation_2021,
  title = {Large-{{Deviation Approach}} to {{Random Recurrent Neuronal Networks}}: {{Parameter Inference}} and {{Fluctuation-Induced Transitions}}},
  shorttitle = {Large-{{Deviation Approach}} to {{Random Recurrent Neuronal Networks}}},
  author = {{van Meegen}, Alexander and K{\"u}hn, Tobias and Helias, Moritz},
  year = {2021},
  month = oct,
  journal = {Phys. Rev. Lett.},
  volume = {127},
  number = {15},
  pages = {158302},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.127.158302},
  urldate = {2022-05-23},
  langid = {english}
}

@article{van_meegen_microscopic_2021,
  title = {Microscopic Theory of Intrinsic Timescales in Spiking Neural Networks},
  author = {{van Meegen}, Alexander and {van Albada}, Sacha J.},
  year = {2021},
  month = oct,
  journal = {Phys. Rev. Res.},
  volume = {3},
  number = {4},
  pages = {043077},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevResearch.3.043077},
  urldate = {2021-12-14},
  abstract = {A complex interplay of single-neuron properties and the recurrent network structure shapes the activity of cortical neurons. The single-neuron activity statistics differ in general from the respective population statistics, including spectra and, correspondingly, autocorrelation times. We develop a theory for self-consistent second-order single-neuron statistics in block-structured sparse random networks of spiking neurons. In particular, the theory predicts the neuron-level autocorrelation times, also known as intrinsic timescales, of the neuronal activity. The theory is based on an extension of dynamic mean-field theory from rate networks to spiking networks, which is validated via simulations. It accounts for both static variability, e.g., due to a distributed number of incoming synapses per neuron, and temporal fluctuations of the input. We apply the theory to balanced random networks of generalized linear model neurons, balanced random networks of leaky integrate-and-fire neurons, and a biologically constrained network of leaky integrate-and-fire neurons. For the generalized linear model network with an error function nonlinearity, a novel analytical solution of the colored noise problem allows us to obtain self-consistent firing rate distributions, single-neuron power spectra, and intrinsic timescales. For the leaky integrate-and-fire networks, we derive an approximate analytical solution of the colored noise problem, based on the Stratonovich approximation of the Wiener-Rice series and a novel analytical solution for the free upcrossing statistics. Again closing the system self-consistently, in the fluctuation-driven regime, this approximation yields reliable estimates of the mean firing rate and its variance across neurons, the interspike-interval distribution, the single-neuron power spectra, and intrinsic timescales. With the help of our theory, we find parameter regimes where the intrinsic timescale significantly exceeds the membrane time constant, which indicates the influence of the recurrent dynamics. Although the resulting intrinsic timescales are on the same order for generalized linear model neurons and leaky integrate-and-fire neurons, the two systems differ fundamentally: for the former, the longer intrinsic timescale arises from an increased firing probability after a spike; for the latter, it is a consequence of a prolonged effective refractory period with a decreased firing probability. Furthermore, the intrinsic timescale attains a maximum at a critical synaptic strength for generalized linear model networks, in contrast to the minimum found for leaky integrate-and-fire networks.}
}

@article{vardi_simultaneous_2016,
  title = {Simultaneous Multi-Patch-Clamp and Extracellular-Array Recordings: {{Single}} Neuron Reflects Network Activity},
  shorttitle = {Simultaneous Multi-Patch-Clamp and Extracellular-Array Recordings},
  author = {Vardi, Roni and Goldental, Amir and Sardi, Shira and Sheinin, Anton and Kanter, Ido},
  year = {2016},
  month = dec,
  journal = {Sci. Rep.},
  volume = {6},
  number = {1},
  pages = {36228},
  issn = {2045-2322},
  doi = {10.1038/srep36228},
  urldate = {2021-09-18},
  langid = {english}
}

@article{vardi_simultaneous_2016-1,
  title = {Simultaneous Multi-Patch-Clamp and Extracellular-Array Recordings: {{Single}} Neuron Reflects Network Activity},
  shorttitle = {Simultaneous Multi-Patch-Clamp and Extracellular-Array Recordings},
  author = {Vardi, Roni and Goldental, Amir and Sardi, Shira and Sheinin, Anton and Kanter, Ido},
  year = {2016},
  month = nov,
  journal = {Sci. Rep.},
  volume = {6},
  number = {1},
  pages = {36228},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep36228},
  urldate = {2022-01-13},
  abstract = {The increasing number of recording electrodes enhances the capability of capturing the network's cooperative activity, however, using too many monitors might alter the properties of the measured neural network and induce noise. Using a technique that merges simultaneous multi-patch-clamp and multi-electrode array recordings of neural networks in-vitro, we show that the membrane potential of a single neuron is a reliable and super-sensitive probe for monitoring such cooperative activities and their detailed rhythms. Specifically, the membrane potential and the spiking activity of a single neuron are either highly correlated or highly anti-correlated with the time-dependent macroscopic activity of the entire network. This surprising observation also sheds light on the cooperative origin of neuronal burst in cultured networks. Our findings present an alternative flexible approach to the technique based on a massive tiling of networks by large-scale arrays of electrodes to monitor their activity.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Biological physics,Complex networks},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Biological physics;Complex networks Subject\_term\_id: biological-physics;complex-networks}
}

@article{vreeken_real-world_nodate,
  title = {On Real-World Temporal Pattern Recognition Using {{Liquid State Machines}}},
  author = {Vreeken, Jilles},
  pages = {73},
  langid = {english}
}

@article{vreeswijk_chaos_1996,
  title = {Chaos in {{Neuronal Networks}} with {{Balanced Excitatory}} and {{Inhibitory Activity}}},
  author = {van Vreeswijk, C. and Sompolinsky, H.},
  year = {1996},
  month = dec,
  journal = {Science},
  volume = {274},
  number = {5293},
  pages = {1724--1726},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.274.5293.1724},
  urldate = {2019-04-17},
  abstract = {Neurons in the cortex of behaving animals show temporally irregular spiking patterns. The origin of this irregularity and its implications for neural processing are unknown. The hypothesis that the temporal variability in the firing of a neuron results from an approximate balance between its excitatory and inhibitory inputs was investigated theoretically. Such a balance emerges naturally in large networks of excitatory and inhibitory neuronal populations that are sparsely connected by relatively strong synapses. The resulting state is characterized by strongly chaotic dynamics, even when the external inputs to the network are constant in time. Such a network exhibits a linear response, despite the highly nonlinear dynamics of single neurons, and reacts to changing external stimuli on time scales much smaller than the integration time constant of a single neuron.},
  copyright = {\textcopyright{} 1996 American Association for the Advancement of Science},
  langid = {english},
  pmid = {8939866}
}

@article{wang_bistability_2019,
  title = {Bistability for {{CO Oxidation}}: {{An Understanding}} from {{Extended Phenomenological Kinetics Simulations}}},
  shorttitle = {Bistability for {{CO Oxidation}}},
  author = {Wang, He and Shen, Tonghao and Duan, Sai and Chen, Zheng and Xu, Xin},
  year = {2019},
  month = dec,
  journal = {ACS Catal.},
  volume = {9},
  number = {12},
  pages = {11116--11124},
  publisher = {{American Chemical Society}},
  doi = {10.1021/acscatal.9b03407},
  urldate = {2022-02-09},
  abstract = {CO oxidation can exhibit diversely appealing catalytic behaviors on metal surfaces. Exploring such properties is crucial not only for fundamental understanding but also for practical applications. In the present work, we focus on bistability and comprehensively investigate how the bistability is influenced by changes of the apparent rate coefficients of the elementary reaction steps. Such changes are related to the changes of reaction conditions and the design of the catalysts. For CO oxidation at certain lower temperatures and total pressures, there exists a bistability region, where one state, the reactive (R-) state, is predominantly oxygen-covered while CO can still adsorb and react, whereas the other state, the P-state, is CO poisoned and O2 adsorption is inhibited and the catalyst is inactive. By means of extended phenomenological kinetics (XPK) simulations, we find that the minimum CO pressure for the system to stay at the P-state is determined by the apparent rate coefficients of CO desorption and oxygen adsorption, meanwhile the maximum CO pressure for the system to stay at the R-state is determined by the apparent rate coefficients of CO oxidation and oxygen adsorption. The XPK results reveal that lateral interactions between adsorbates can significantly affect the rate coefficients and thus have a strong impact on the bistability region. With these understandings, we further predicted that the sandwiched PdRuPd(100), PdOsPd(100), and PdCoPd(100) surfaces would be good candidates as catalytic converters to reduce CO pollution from automobile engines under both conditions before and after the catalysts are warmed up to the ``ignition'' temperature.}
}

@article{wasmuht_intrinsic_2018,
  title = {Intrinsic Neuronal Dynamics Predict Distinct Functional Roles during Working Memory},
  author = {Wasmuht, D. F. and Spaak, E. and Buschman, T. J. and Miller, E. K. and Stokes, M. G.},
  year = {2018},
  month = aug,
  journal = {Nat. Commun.},
  volume = {9},
  number = {1},
  pages = {3499},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-05961-4},
  urldate = {2022-02-01},
  abstract = {Working memory (WM) is characterized by the ability to maintain stable representations over time; however, neural activity associated with WM maintenance can be highly dynamic. We explore whether complex population coding dynamics during WM relate to the intrinsic temporal properties of single neurons in lateral prefrontal cortex (lPFC), the frontal eye fields (FEF), and lateral intraparietal cortex (LIP) of two monkeys (Macaca mulatta). We find that cells with short timescales carry memory information relatively early during memory encoding in lPFC; whereas long-timescale cells play a greater role later during processing, dominating coding in the delay period. We also observe a link between functional connectivity at rest and the intrinsic timescale in FEF and LIP. Our results indicate that individual differences in the temporal processing capacity predict complex neuronal dynamics during WM, ranging from rapid dynamic encoding of stimuli to slower, but stable, maintenance of mnemonic information.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Attention,Cognitive control,Working memory}
}

@article{wilson_up_2008,
  title = {Up and down States},
  author = {Wilson, Charles},
  year = {2008},
  month = jun,
  journal = {Scholarpedia},
  volume = {3},
  number = {6},
  pages = {1410},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.1410},
  urldate = {2022-01-19},
  langid = {english}
}

@article{wilting_25_2019,
  title = {25 Years of Criticality in Neuroscience \textemdash{} Established Results, Open Controversies, Novel Concepts},
  author = {Wilting, J and Priesemann, V},
  year = {2019},
  month = oct,
  journal = {Curr. Opin. Neurobiol.},
  series = {Computational {{Neuroscience}}},
  volume = {58},
  pages = {105--111},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2019.08.002},
  urldate = {2020-02-21},
  abstract = {Twenty-five years ago, Dunkelmann and Radons (1994) showed that neural networks can self-organize to a critical state. In models, the critical state offers a number of computational advantages. Thus this hypothesis, and in particular the experimental work by Beggs and Plenz (2003), has triggered an avalanche of research, with thousands of studies referring to it. Nonetheless, experimental results are still contradictory. How is it possible, that a hypothesis has attracted active research for decades, but nonetheless remains controversial? We discuss the experimental and conceptual controversy, and then present a parsimonious solution that (i) unifies the contradictory experimental results, (ii) avoids disadvantages of a critical state, and (iii) enables rapid, adaptive tuning of network properties to task requirements.},
  langid = {english}
}

@article{wilting_between_2019,
  title = {Between {{Perfectly Critical}} and {{Fully Irregular}}: {{A Reverberating Model Captures}} and {{Predicts Cortical Spike Propagation}}},
  shorttitle = {Between {{Perfectly Critical}} and {{Fully Irregular}}},
  author = {Wilting, J. and Priesemann, V.},
  year = {2019},
  month = jun,
  journal = {Cereb. Cortex},
  volume = {29},
  number = {6},
  pages = {2759--2770},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhz049},
  urldate = {2019-08-06},
  abstract = {Abstract.  Knowledge about the collective dynamics of cortical spiking is very informative about the underlying coding principles. However, even most basic prop},
  langid = {english}
}

@article{wilting_operating_2018,
  title = {Operating in a {{Reverberating Regime Enables Rapid Tuning}} of {{Network States}} to {{Task Requirements}}},
  author = {Wilting, Jens and Dehning, Jonas and Pinheiro Neto, Joao and Rudelt, Lucas and Wibral, Michael and Zierenberg, Johannes and Priesemann, Viola},
  year = {2018},
  journal = {Front. Syst. Neurosci.},
  volume = {12},
  pages = {55},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2018.00055},
  urldate = {2018-11-19},
  abstract = {Neural circuits are able to perform computations under very diverse conditions and requirements. The required computations impose clear constraints on their fine-tuning: a rapid and maximally informative response to stimuli in general requires decorrelated baseline neural activity. Such network dynamics is known as asynchronous-irregular. In contrast, spatio-temporal integration of information requires maintenance and transfer of stimulus information over extended time periods. This can be realized at criticality, a phase transition where correlations, sensitivity and integration time diverge. Being able to flexibly switch, or even combine the above properties in a task-dependent manner would present a clear functional advantage. We propose that cortex operates in a reverberating regime because it is particularly favorable for ready adaptation of computational properties to context and task. This reverberating regime enables cortical networks to interpolate between the asynchronous-irregular and the critical state by small changes in effective synaptic strength or excitation-inhibition ratio. These changes directly adapt computational properties, including sensitivity, amplification, integration time and correlation length within the local network. We review recent converging evidence that cortex in vivo operates in the reverberating regime, and that various cortical areas have adapted their integration times to processing requirements. In addition, we propose that neuromodulation enables a fine-tuning of the network, so that local circuits can either decorrelate or integrate, and quench or maintain their input depending on task. We argue that this task-dependent tuning, which we call dynamic adaptive computation, presents a central organization principle of cortical networks and discuss first experimental evidence.},
  langid = {english},
  keywords = {balanced state,Cognitive states,Criticality,Dynamical state,hierarchy,network function,Neural Network,Neuromodulation}
}

@article{wu_homeostatic_2020,
  title = {Homeostatic Mechanisms Regulate Distinct Aspects of Cortical Circuit Dynamics},
  author = {Wu, Yue Kris and Hengen, Keith B. and Turrigiano, Gina G. and Gjorgjieva, Julijana},
  year = {2020},
  month = sep,
  journal = {Proc. Natl. Acad. Sci.},
  volume = {117},
  number = {39},
  pages = {24514--24525},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1918368117},
  urldate = {2021-11-24},
  abstract = {Homeostasis is indispensable to counteract the destabilizing effects of Hebbian plasticity. Although it is commonly assumed that homeostasis modulates synaptic strength, membrane excitability, and firing rates, its role at the neural circuit and network level is unknown. Here, we identify changes in higher-order network properties of freely behaving rodents during prolonged visual deprivation. Strikingly, our data reveal that functional pairwise correlations and their structure are subject to homeostatic regulation. Using a computational model, we demonstrate that the interplay of different plasticity and homeostatic mechanisms can capture the initial drop and delayed recovery of firing rates and correlations observed experimentally. Moreover, our model indicates that synaptic scaling is crucial for the recovery of correlations and network structure, while intrinsic plasticity is essential for the rebound of firing rates, suggesting that synaptic scaling and intrinsic plasticity can serve distinct functions in homeostatically regulating network dynamics.},
  langid = {english}
}

@article{zeraati_self-organization_2021,
  title = {Self-Organization toward Criticality by Synaptic Plasticity},
  author = {Zeraati, Roxana and Priesemann, Viola and Levina, Anna},
  year = {2021},
  month = apr,
  journal = {Front. Phys.},
  volume = {9},
  pages = {619661},
  issn = {2296-424X},
  doi = {10.3389/fphy.2021.619661},
  urldate = {2022-02-28},
  abstract = {Self-organized criticality has been proposed to be a universal mechanism for the emergence of scale-free dynamics in many complex systems, and possibly in the brain. While such scale-free patterns were identified experimentally in many different types of neural recordings, the biological principles behind their emergence remained unknown. Utilizing different network models and motivated by experimental observations, synaptic plasticity was proposed as a possible mechanism to self-organize brain dynamics towards a critical point. In this review, we discuss how various biologically plausible plasticity rules operating across multiple timescales are implemented in the models and how they alter the network's dynamical state through modification of number and strength of the connections between the neurons. Some of these rules help to stabilize criticality, some need additional mechanisms to prevent divergence from the critical state. We propose that rules that are capable of bringing the network to criticality can be classified by how long the near-critical dynamics persists after their disabling. Finally, we discuss the role of self-organization and criticality in computation. Overall, the concept of criticality helps to shed light on brain function and self-organization, yet the overall dynamics of living neural networks seem to harnesses not only criticality for computation, but also deviations thereof.},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Physics - Biological Physics,Quantitative Biology - Neurons and Cognition}
}

@article{zierenberg_canonical_2017,
  title = {Canonical Free-Energy Barrier of Particle and Polymer Cluster Formation},
  author = {Zierenberg, Johannes and Schierz, Philipp and Janke, Wolfhard},
  year = {2017},
  month = feb,
  journal = {Nat. Commun.},
  volume = {8},
  pages = {14546},
  issn = {2041-1723},
  doi = {10.1038/ncomms14546},
  urldate = {2019-03-12},
  langid = {english}
}

@article{zierenberg_homeostatic_2018,
  title = {Homeostatic {{Plasticity}} and {{External Input Shape Neural Network Dynamics}}},
  author = {Zierenberg, Johannes and Wilting, Jens and Priesemann, Viola},
  year = {2018},
  month = jul,
  journal = {Phys. Rev. X},
  volume = {8},
  number = {3},
  pages = {031018},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.8.031018},
  urldate = {2018-11-05},
  langid = {english}
}

@article{zierenberg_notitle_nodate,
  author = {Zierenberg, Johannes and Buend{\'i}a, Victor and Cramer, Benjamin and Priesemann, Viola and Mu{\~n}oz, Miguel A.},
  journal = {In preparation}
}

@article{zierenberg_tailored_2020,
  title = {Tailored Ensembles of Neural Networks Optimize Sensitivity to Stimulus Statistics},
  author = {Zierenberg, Johannes and Wilting, Jens and Priesemann, Viola and Levina, Anna},
  year = {2020},
  month = feb,
  journal = {Phys. Rev. Res.},
  volume = {2},
  number = {1},
  pages = {013115},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevResearch.2.013115},
  urldate = {2022-01-03},
  abstract = {The capability of a living organism to process stimuli with nontrivial intensity distributions cannot be explained by the proficiency of a single neural network. Moreover, it is not sufficient to maximize the dynamic range of the neural response; it is also necessary to tune the response to the intervals of stimulus intensities that should be reliably discriminated. We derive a class of neural networks where these intervals can be tuned to the desired interval. This allows us to tailor ensembles of networks optimized for arbitrary stimulus intensity distributions. We discuss potential applications in machine learning.}
}

@article{zierenberg_tailored_2020-1,
  title = {Tailored Ensembles of Neural Networks Optimize Sensitivity to Stimulus Statistics},
  author = {Zierenberg, Johannes and Wilting, Jens and Priesemann, Viola and Levina, Anna},
  year = {2020},
  month = feb,
  journal = {Phys. Rev. Res.},
  volume = {2},
  number = {1},
  pages = {013115},
  doi = {10.1103/PhysRevResearch.2.013115},
  urldate = {2020-02-05},
  abstract = {The capability of a living organism to process stimuli with nontrivial intensity distributions cannot be explained by the proficiency of a single neural network. Moreover, it is not sufficient to maximize the dynamic range of the neural response; it is also necessary to tune the response to the intervals of stimulus intensities that should be reliably discriminated. We derive a class of neural networks where these intervals can be tuned to the desired interval. This allows us to tailor ensembles of networks optimized for arbitrary stimulus intensity distributions. We discuss potential applications in machine learning.}
}
